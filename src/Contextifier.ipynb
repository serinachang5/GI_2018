{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n",
      "Initializing TweetLevel...\n",
      "Number of embeddings in ../data/w2v_word_s300_w5_mc5_it20.bin: 23417\n",
      "Sample tweet_dict item: (740043438788345856, [2, 254, 440, 192, 94, 57, 72, 77])\n",
      "Size of tweet_dict: 1033655\n",
      "Initializing TweetLevel...\n",
      "Number of embeddings in ../data/splex_standard_svd_word_s300_seeds_hc.pkl: 20000\n",
      "Sample tweet_dict item: (740043438788345856, [2, 254, 440, 192, 94, 57, 72, 77])\n",
      "Size of tweet_dict: 1033655\n"
     ]
    }
   ],
   "source": [
    "# necessary outside things\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import Data_loader\n",
    "option = 'word'\n",
    "max_len = 20\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)\n",
    "\n",
    "from represent_tweet_level import TweetLevel\n",
    "tl_w2v = TweetLevel(word_level='../data/w2v_word_s300_w5_mc5_it20.bin', wl_file_type='w2v')\n",
    "tl_splex = TweetLevel(word_level='../data/splex_standard_svd_word_s300_seeds_hc.pkl', wl_file_type='pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_id = 905716028390481921\n",
    "w_emb = tl_w2v.get_representation(tweet_id, mode='avg')\n",
    "sp_emb =  tl_splex.get_representation(tweet_id, mode='avg')\n",
    "full_emb = np.concatenate([w_emb, sp_emb])\n",
    "print(len(full_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contextifier:\n",
    "    '''\n",
    "        Creates the context for tweets.\n",
    "    '''\n",
    "    def __init__(self, context_size=1, context_combine='avg', use_rt_user=False, \n",
    "                 use_mentions=False, use_rt_mentions=False, context_hl=1.0):\n",
    "        '''\n",
    "        Create it!\n",
    "        Args:\n",
    "            context_size (int): Number of days to look back\n",
    "            context_combine (str): Method of combining tweet embeddings of tweets in context\n",
    "            use_rt_user (bool): User A retweets User B's tweet -- if true,\n",
    "                    this tweet will be counted in User A and User B's context\n",
    "            use_mentions (bool): User A tweets, mentioning User B -- if true, \n",
    "                    this tweet will be in User A and User B's context\n",
    "            use_rt_mentions (bool): User A retweets User B's tweet, which mentioned User C -- if true,\n",
    "                    this tweet will counted in User A and User C's history\n",
    "            context_hl (int): Half life of context, in days. Tweet embeddings will be weighed according to\n",
    "                    (self.decay_rate)^(t/context_hl) where t is the number of days the previous tweet is \n",
    "                    from the current one.\n",
    "        '''\n",
    "        \n",
    "        # need data loader eventually?\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.context_combine = context_combine\n",
    "        self.use_rt_user = use_rt_user\n",
    "        self.use_mentions = use_mentions\n",
    "        self.use_rt_mentions = use_rt_mentions\n",
    "        self.context_hl = context_hl\n",
    "        self.decay_rate = 0.5 # hardcoded!\n",
    "        \n",
    "        self.user_ct_tweets = {}\n",
    "        self.all_data = dl.all_data()\n",
    "        \n",
    "        # Map from tweet id to tuple of (user, idx in sorted list)\n",
    "        # Note that \"user\" is user_post, the user who posted the tweet\n",
    "        self.id_to_location = {}\n",
    "        \n",
    "        # Tweet to context embedding\n",
    "        self.tweet_to_ct = {}\n",
    "        \n",
    "        # Cache for calculated tweet embeddings\n",
    "        self.tweet_emb_cache = {}\n",
    "        \n",
    "        # Hardcoding dimension to 303 (300 w2v, 3 splex) -- remove later\n",
    "        self.embeddings_dim = 300 + 3\n",
    "    \n",
    "    \n",
    "    def create_user_context_tweets(self):\n",
    "        ''' Describe! '''\n",
    "        \n",
    "        # For every tweet in the dataset (labled and unlabeled)\n",
    "        for tweet in self.all_data:\n",
    "            incl_users = set()\n",
    "            # Always include poster\n",
    "            incl_users.add(tweet['user_post'])\n",
    "            # Check if tweet is a retweet\n",
    "            if 'user_retweet' in tweet:\n",
    "                # Include retweeted user\n",
    "                if self.use_rt_user:\n",
    "                    incl_users.add(tweet['user_retweet'])\n",
    "                # Include users mentioned in retweet\n",
    "                if use_rt_mentions:\n",
    "                    incl_users.union(tweet['user_mentions'])\n",
    "            # Include mentioned users (non-retweet case)\n",
    "            elif use_mentions:\n",
    "                incl_users.union(tweet['user_mentions'])\n",
    "            \n",
    "            # Add tweets to users' context tweets\n",
    "            for u in incl_users:\n",
    "                if u in self.user_ct_tweets:\n",
    "                    self.user_ct_tweets[u].append(tweet)\n",
    "                else:\n",
    "                    self.user_ct_tweets[u] = [tweet]\n",
    "        \n",
    "        # Sort context tweets chronologically\n",
    "        for u in self.user_ct_tweets:\n",
    "            self.user_ct_tweets[u] = sorted(self.user_ct_tweets[u], key=lambda t: t['created_at'])\n",
    "            \n",
    "        # Go through the tweets to save their location\n",
    "        for u, tweets in self.user_ct_tweets.items():\n",
    "            for idx, t in enumerate(tweets):\n",
    "                if u == t['user_post']:\n",
    "                    self.id_to_location[t['tweet_id']] = (u, idx)\n",
    "    \n",
    "    \n",
    "    def get_tweet_embedding(self, tweet_id):\n",
    "        '''\n",
    "        Get the tweet embedding for the given tweet.\n",
    "        Args:\n",
    "            tweet_id (int): the id of the tweet, according to twitter's ID system\n",
    "        Returns:\n",
    "            the tweet embedding\n",
    "        '''\n",
    "        if tweet_id in self.tweet_emb_cache: # Check cache for embedding\n",
    "            return self.tweet_emb_cache[tweet_id]\n",
    "        else:\n",
    "            w_emb = tl_w2v.get_representation(tweet_id, mode='avg')\n",
    "            sp_emb =  tl_splex.get_representation(tweet_id, mode='avg')\n",
    "            full_emb = np.concatenate([w_emb, sp_emb])\n",
    "            self.tweet_emb_cache[tweet_id] = full_emb # Save embedding to cache\n",
    "            return full_emb\n",
    "    \n",
    "    \n",
    "    def create_context_embedding(self, user_id, tweet_idx):\n",
    "        '''\n",
    "        Get the context embedding for the given tweet, determined by user and index.\n",
    "        Args:\n",
    "            user_id (int): the id of the user, according to data_loader's user ids\n",
    "            tweet_idx (int): the index of the tweet in self.user_ct_tweets[user_id]\n",
    "        '''\n",
    "        # Check if context embedding is in the cache\n",
    "        tweet_id = self.user_ct_tweets[user_id][tweet_idx]['tweet_id']\n",
    "        if tweet_id in self.tweet_to_ct:\n",
    "            return self.tweet_to_ct[tweet_id]\n",
    "        \n",
    "        # Return difference in days, as a float\n",
    "        def days_diff(d1, d2):\n",
    "            return (d1 - d2).seconds/60/60/24\n",
    "        \n",
    "        tweet_embs = []\n",
    "        \n",
    "        today = self.user_ct_tweets[user_id][tweet_idx]['created_at']\n",
    "        i = tweet_idx-1\n",
    "        while i >= 0 and days_diff(today, self.user_ct_tweets[user_id][i]['created_at']) \\\n",
    "                                     < self.context_size:\n",
    "            # Get embedding -- may need to change\n",
    "            emb = self.get_tweet_embedding(self.user_ct_tweets[user_id][i]['tweet_id'])\n",
    "            # Weigh embedding\n",
    "            diff = days_diff(today, self.user_ct_tweets[user_id][i]['created_at'])\n",
    "            weight = self.decay_rate ** (diff/self.context_hl)\n",
    "            emb = emb * weight\n",
    "            # Save\n",
    "            tweet_embs.append(emb)\n",
    "            i -= 1\n",
    "        \n",
    "        result = None\n",
    "        if len(tweet_embs) == 0:\n",
    "            result = np.zeros(self.embeddings_dim, )\n",
    "        else:\n",
    "            if self.context_combine == 'avg':\n",
    "                result = np.mean(np.array(tweet_embs), axis=0)\n",
    "            elif self.context_combine == 'sum':\n",
    "                result = sum(tweet_embs)\n",
    "            elif self.context_combine == 'max':\n",
    "                result = np.max(np.array(tweet_embs), axis=0)\n",
    "            else:\n",
    "                raise ValueError('Unknown settting for context_combine:', context_combine)\n",
    "        \n",
    "        # Cache the result\n",
    "        self.tweet_to_ct[tweet_id] = result\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def create_context_embeddings(self):\n",
    "        '''\n",
    "        Create the context embeddings for the tweets.\n",
    "        '''\n",
    "        self.tweet_to_ct = {} # Reset embeddings\n",
    "        \n",
    "        for fold_idx in range(0, 5): # change to 5\n",
    "            tr, val, test = dl.cv_data(fold_idx)\n",
    "            all_tweets = [t for l in [tr, val, test] for t in l ]\n",
    "            print(len(all_tweets))\n",
    "            for tweet in all_tweets: \n",
    "                self.tweet_to_ct[tweet['tweet_id']] = self.create_context_embedding(\n",
    "                    *self.id_to_location[tweet['tweet_id']])\n",
    "            print('done with:', fold_idx)\n",
    "    \n",
    "    \n",
    "    def get_context_embedding(self, tweet_id):\n",
    "        '''\n",
    "        Get the context embedding for the specified tweet, determined by tweet_id\n",
    "        Args:\n",
    "            tweet_id (int): the id of the tweet, according to the twitter tweet ids\n",
    "        Returns:\n",
    "            (np.array(int)): the context embedding \n",
    "        '''\n",
    "        if len(self.tweet_to_ct) == 0:\n",
    "            raise ValueError('Context embeddings have not been created yet. Call create_context_embeddings().')\n",
    "        if tweet_id not in self.tweet_to_ct:\n",
    "            raise ValueError('No calcualted context embedding for given tweet_id:', tweet_id)\n",
    "        \n",
    "        return self.tweet_to_ct[tweet_id]\n",
    "\n",
    "    \n",
    "    def from_file(self, in_file):\n",
    "        '''\n",
    "        Reads the context embeddings in from a file.\n",
    "        Args:\n",
    "            in_file (str): the path to the file, in csv format, <tweet_id>, <embedding>\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        with open(in_file, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.tweet_to_ct[int(row['tweet_id'])] = np.fromstring(row['context_embedding'],\n",
    "                                                                    dtype=float, sep=' ')\n",
    "        \n",
    "\n",
    "    \n",
    "    def write_context_embeddings(self, out_file=None):\n",
    "        '''\n",
    "        Writes the embeddings to a file.\n",
    "        Args:\n",
    "            out_file (str): the path of the file to write to\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        if not out_file:\n",
    "            out_file = 'context_emb_{0}_{1}_rt{2}_men{3}_rtmen{4}_hl{5}.csv' \\\n",
    "                        .format(self.context_size, self.context_combine, self.use_rt_user, \n",
    "                                self.use_mentions, self.use_rt_mentions, self.context_hl)\n",
    "        with open(out_file, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(['tweet_id', 'context_embedding'])\n",
    "            for tweet_id, ct_emb in self.tweet_to_ct.items():\n",
    "                ct_emb_str = ' '.join([str(x) for x in ct_emb])\n",
    "                writer.writerow([tweet_id, ct_emb_str])\n",
    "                \n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester/usage\n",
    "\n",
    "# number of days\n",
    "context_size = 5 \n",
    "\n",
    "# method of combining tweet embeddings\n",
    "context_combine = 'avg' \n",
    "\n",
    "# User A retweets User B's tweet -- if true: the retweet will be counted in both User A and User B's context\n",
    "use_rt_user = False\n",
    "\n",
    "# User A tweets, mentioning User B -- if true: this tweet will be in User A and User B's context\n",
    "use_mentions = True\n",
    "\n",
    "# User A retweets User B's tweet, which originally mentioned User C -- if true: counted in A, B and C's history\n",
    "use_rt_mentions = False\n",
    "\n",
    "# the data loader\n",
    "data_loader = dl\n",
    "\n",
    "# will eventually take params, but for now all we need is the data loader\n",
    "contextifier = Contextifier(context_size, context_combine, use_rt_user, use_mentions, use_rt_mentions, context_hl=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextifier.create_user_context_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7842\n",
      "done with: 0\n",
      "7842\n",
      "done with: 1\n",
      "7842\n",
      "done with: 2\n",
      "7842\n",
      "done with: 3\n",
      "7842\n",
      "done with: 4\n"
     ]
    }
   ],
   "source": [
    "contextifier.create_context_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'created_at': datetime.datetime(2012, 9, 10, 18, 31, 39), 'user_mentions': [6083], 'user_post': 25, 'tweet_id': 245228056422252544, 'int_arr': [2, 183], 'padded_int_arr': [2, 183, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'created_at': datetime.datetime(2012, 10, 17, 20, 13, 41), 'user_mentions': [7831], 'user_post': 25, 'tweet_id': 258662084089368576, 'int_arr': [2, 16, 60, 10, 219, 259, 16, 142, 538], 'padded_int_arr': [2, 16, 60, 10, 219, 259, 16, 142, 538, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]\n"
     ]
    }
   ],
   "source": [
    "print(len(contextifier.tweet_to_ct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "contextifier.get_context_embedding(245228056422252544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextifier.write_context_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextifier2 = Contextifier()\n",
    "contextifier2.from_file('context_emb_5_avg_rtFalse_menTrue_rtmenFalse_hl1.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.29765159e-01, -7.57820561e-02, -2.33109747e-02, -7.06422932e-02,\n",
       "       -2.27849730e-02, -1.07100463e-01, -1.65977761e-02,  1.44108103e-01,\n",
       "       -3.63823417e-02,  5.68953571e-02,  1.14796710e-01,  1.33025157e-01,\n",
       "        2.92387255e-02, -3.38103699e-02, -7.10422493e-02, -1.16686263e-01,\n",
       "       -4.57345906e-02, -1.24062378e-01,  3.16525381e-02,  5.46709085e-02,\n",
       "        2.09999887e-01,  4.17215812e-02,  2.01643577e-01, -5.36820738e-03,\n",
       "        9.94055704e-02,  4.52494678e-02,  1.11036305e-01,  1.02455396e-01,\n",
       "        3.17793973e-02,  1.68900885e-01, -2.67782665e-01, -6.24724609e-02,\n",
       "       -1.28560938e-02, -1.03814518e-01,  1.24277295e-01, -9.12616807e-03,\n",
       "        8.69174185e-02,  7.29920363e-02, -5.31217368e-02, -1.70796777e-01,\n",
       "       -1.01166716e-01, -9.64308399e-02, -7.73154602e-02,  3.18801045e-02,\n",
       "        1.39264479e-02, -7.16256419e-02, -4.30586126e-02,  3.28289078e-02,\n",
       "        1.23525254e-01,  2.21787181e-02, -4.53831046e-02, -3.42465475e-02,\n",
       "        2.02838960e-02,  1.72125649e-01,  1.48446832e-01,  9.55721648e-02,\n",
       "       -7.07447163e-02,  9.20525687e-02,  1.11525725e-01, -8.05130756e-02,\n",
       "        5.32518741e-02,  7.39210691e-02, -9.47900567e-02, -1.97842388e-01,\n",
       "        7.74945489e-02,  4.80096411e-02,  1.12402317e-01,  1.17273831e-01,\n",
       "       -3.01488165e-01, -1.10072826e-01, -8.61139496e-02, -4.57733859e-02,\n",
       "       -2.59093626e-02,  6.99099922e-02, -4.13044517e-02, -6.25556806e-03,\n",
       "       -3.42758275e-02,  1.27982958e-01, -1.56054807e-01, -5.62159774e-02,\n",
       "        4.01637083e-02,  1.23888629e-01,  3.07298387e-02, -6.59335315e-02,\n",
       "       -6.56563983e-02, -2.12227727e-02,  2.40146105e-01,  1.16017009e-01,\n",
       "        3.67183827e-02, -2.05417103e-02,  1.40221312e-01, -1.48582446e-01,\n",
       "        1.72147782e-02, -1.29100812e-01,  1.05346382e-01,  1.75295112e-02,\n",
       "       -6.78505003e-02,  2.72243662e-01, -2.74253377e-02,  1.48715856e-01,\n",
       "       -1.52194659e-01,  3.97376739e-02, -1.04819942e-01, -7.07697629e-02,\n",
       "       -7.73751400e-02,  9.69425047e-02, -5.83548700e-02, -1.62998279e-01,\n",
       "       -1.63529314e-01,  7.99274588e-02,  7.26400958e-02, -1.49324852e-01,\n",
       "        9.30258158e-02,  5.57698134e-02,  4.36009933e-02, -2.76329070e-02,\n",
       "       -1.15417093e-02,  1.13580142e-02, -2.32799102e-01,  4.13603829e-02,\n",
       "       -3.68282433e-02,  1.21680570e-01,  7.87181895e-03,  7.96860707e-02,\n",
       "        5.62054218e-02, -6.00850428e-02,  6.63201399e-02,  1.17371977e-01,\n",
       "        9.84341188e-04, -2.14084772e-01,  7.41856740e-04,  1.25832110e-01,\n",
       "       -2.61984792e-03, -9.36645336e-02, -5.95095405e-02, -3.84860874e-02,\n",
       "        1.79385675e-01,  1.65115637e-01,  3.71784973e-02,  2.06140472e-01,\n",
       "       -2.71668985e-02, -6.80762045e-02,  1.78208777e-02,  3.71885890e-02,\n",
       "       -1.13187084e-02,  1.57036528e-02,  5.43698979e-02,  1.78472896e-01,\n",
       "       -2.99548056e-02,  7.87377670e-02, -8.54883744e-02,  1.24885956e-01,\n",
       "        5.37029785e-02,  8.70808695e-02,  1.81715906e-01,  7.12005109e-02,\n",
       "        1.53446272e-01, -6.54682170e-02, -6.54007617e-02, -1.34755022e-01,\n",
       "        4.88953871e-03,  1.50963791e-01,  4.24610888e-02, -2.76850334e-01,\n",
       "        2.58553084e-01,  7.16784301e-02,  1.94594397e-01, -1.92980673e-01,\n",
       "       -9.18748954e-02, -1.50827350e-01,  1.61901168e-01, -3.60668040e-02,\n",
       "        9.27312353e-02,  2.76018772e-02, -1.15142902e-01, -7.66211445e-02,\n",
       "       -2.11841044e-02, -3.59775406e-02, -4.83323972e-03, -2.14522987e-02,\n",
       "       -8.27938181e-02,  1.66747360e-01,  1.46146002e-01, -4.38322572e-02,\n",
       "        6.06716946e-02,  2.72083132e-02,  8.21704132e-02,  8.99589591e-02,\n",
       "       -6.42416551e-02, -1.14596758e-01,  3.22074472e-02, -6.61766280e-02,\n",
       "       -4.19630581e-02, -1.15709440e-01,  1.36922109e-01,  1.56508034e-02,\n",
       "        2.81338056e-02,  2.82569033e-02, -5.58558415e-02,  5.39145805e-02,\n",
       "        1.02450569e-01,  1.33684466e-02,  3.53651972e-02, -7.04219997e-02,\n",
       "       -4.56936815e-02, -9.69847834e-02,  2.21213201e-01, -8.64124725e-02,\n",
       "        6.88892421e-02,  2.36619298e-02,  1.00213895e-01, -2.45533491e-02,\n",
       "       -3.08560448e-02,  1.80202290e-01,  1.48377296e-01,  1.06012288e-01,\n",
       "       -3.80284923e-02,  1.56124056e-02,  1.20060659e-01, -9.05493979e-02,\n",
       "        2.62827799e-02,  3.64427017e-02,  8.47545411e-02,  8.47046103e-02,\n",
       "       -1.96431237e-02,  4.49945880e-02,  1.42817306e-01,  4.71737393e-02,\n",
       "       -1.48462249e-01, -1.45746961e-01,  1.57229783e-01,  1.23956571e-01,\n",
       "       -2.24338161e-02,  1.11463604e-02, -3.46151365e-02,  4.33246382e-02,\n",
       "        2.84733711e-01,  1.06596226e-01,  1.62596434e-02,  9.83985445e-02,\n",
       "       -5.24036717e-02,  5.45716657e-02,  1.23162146e-01,  6.93548071e-02,\n",
       "        4.35348889e-02, -1.04897325e-01,  6.96652177e-02,  4.28922251e-03,\n",
       "       -2.59055102e-02, -5.23210168e-02, -1.38118110e-01, -5.95010626e-02,\n",
       "        6.97160439e-02, -1.47329111e-01, -2.86586397e-02, -2.68634275e-01,\n",
       "        7.45149056e-02,  6.77970006e-02, -4.80275393e-02, -9.17573547e-02,\n",
       "       -1.84978837e-03,  6.81567861e-02, -1.60761180e-01,  2.83077856e-02,\n",
       "       -2.27941740e-02, -1.55856787e-03, -1.13530120e-01,  2.24890919e-02,\n",
       "        3.13165083e-02, -2.15054126e-02, -3.67180554e-02, -9.23815775e-02,\n",
       "       -1.12364690e-01, -5.42920749e-02,  9.28359435e-03,  5.52820533e-02,\n",
       "       -4.76019236e-02, -1.50286003e-01,  1.46927179e-01, -6.56355018e-02,\n",
       "        6.12077903e-02, -1.56435712e-02, -2.01244072e-02, -1.40014720e-01,\n",
       "       -1.66041991e-01,  2.27845338e-01,  1.29629364e-02,  3.07891442e-01,\n",
       "       -6.28347065e-02,  7.33610586e-02,  4.24389264e-02,  1.76251917e-02,\n",
       "        1.12329507e-01,  2.11192134e-02,  7.59319718e-02,  2.03967029e-01,\n",
       "        5.90001368e-02,  2.88194955e-01, -2.07456650e-02, -1.18588818e-03,\n",
       "        8.01182701e-01,  4.64479540e-01,  8.23166072e-02])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextifier2.get_context_embedding(832351449069846528)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
