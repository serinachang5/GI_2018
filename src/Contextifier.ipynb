{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data loader\n",
    "from data_loader import Data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading user information finished\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# word level tokenization\n",
    "option = 'word'\n",
    "max_len = 20\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'help', 'otheruser', 'noway'}\n"
     ]
    }
   ],
   "source": [
    "from preprocess import extract_mentioned_user_name\n",
    "print(extract_mentioned_user_name('RT @TyquanAssassin: im waiting @otheruser @help @noway !'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tyquanassassi\n"
     ]
    }
   ],
   "source": [
    "from preprocess import extract_user_rt\n",
    "print(extract_user_rt('RT @TyquanAssassin: im waiting @otheruser !'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_id': 2955996447, 'user_post': 27, 'tweet_id': 740043438788345856, 'user_mentions': [3613], 'created_at': datetime.datetime(2016, 6, 7, 4, 51, 19), 'int_arr': [2, 254, 440, 192, 94, 57, 72, 77], 'padded_int_arr': [2, 254, 440, 192, 94, 57, 72, 77, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(dl.all_data()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_to_tweets = {}\n",
    "for tweet in dl.all_data():\n",
    "    if tweet['user_post'] in user_to_tweets:\n",
    "        user_to_tweets[tweet['user_post']].append(tweet)\n",
    "    else:\n",
    "        user_to_tweets[tweet['user_post']] = [tweet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712\n"
     ]
    }
   ],
   "source": [
    "print(len(user_to_tweets[27]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freekerryhoe\n"
     ]
    }
   ],
   "source": [
    "print(dl.id2user_name(27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in user_to_tweets:\n",
    "    user_to_tweets[u] = sorted(user_to_tweets[u], key=lambda t: t['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = dl.tweets_by_user(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8712\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading user information finished\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "# necessary outside things\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "from data_loader import Data_loader\n",
    "option = 'word'\n",
    "max_len = 20\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)\n",
    "\n",
    "#wv = KeyedVectors.load_word2vec_format(fname, binary=True)\n",
    "\n",
    "\n",
    "class Contextifier:\n",
    "    '''\n",
    "        Creates the context for tweets.\n",
    "    '''\n",
    "    def __init__(self, context_size, context_combine, use_rt_user, use_mentions, use_rt_mentions, context_hl=1.0):\n",
    "        '''\n",
    "        Create it!\n",
    "        Args:\n",
    "            context_size (int): Number of days to look back\n",
    "            context_combine (str): Method of combining tweet embeddings of tweets in context\n",
    "            use_rt_user (bool): User A retweets User B's tweet -- if true,\n",
    "                    this tweet will be counted in User A and User B's context\n",
    "            use_mentions (bool): User A tweets, mentioning User B -- if true, \n",
    "                    this tweet will be in User A and User B's context\n",
    "            use_rt_mentions (bool): User A retweets User B's tweet, which mentioned User C -- if true,\n",
    "                    this tweet will counted in User A and User C's history\n",
    "            context_hl (int): Half life of context, in days. Tweet embeddings will be weighed according to\n",
    "                    (self.decay_rate)^(t/context_hl) where t is the number of days the previous tweet is \n",
    "                    from the current one.\n",
    "        '''\n",
    "        \n",
    "        # need data loader eventually?\n",
    "        \n",
    "        self.context_size = context_size\n",
    "        self.context_combine = context_combine\n",
    "        self.use_rt_user = use_rt_user\n",
    "        self.use_mentions = use_mentions\n",
    "        self.use_rt_mentions = use_rt_mentions\n",
    "        self.context_hl = context_hl\n",
    "        self.decay_rate = 0.5 # hardcoded!\n",
    "        \n",
    "        self.user_ct_tweets = {}\n",
    "        self.all_data = dl.all_data()\n",
    "        \n",
    "        # Tweet to context embedding\n",
    "        self.tweet_to_ct = {}\n",
    "        \n",
    "        # Hardcoding dimension to 300 -- remove later\n",
    "        self.embeddings_dim = 300\n",
    "    \n",
    "    \n",
    "    def create_user_context_tweets(self):\n",
    "        ''' Describe! '''\n",
    "        \n",
    "        # For every tweet in the dataset (labled and unlabeled)\n",
    "        for tweet in self.all_data:\n",
    "            incl_users = set()\n",
    "            # Always include poster\n",
    "            incl_users.add(tweet['user_post'])\n",
    "            # Check if tweet is a retweet\n",
    "            if 'user_retweet' in tweet:\n",
    "                # Include retweeted user\n",
    "                if self.use_rt_user:\n",
    "                    incl_users.add(tweet['user_retweet'])\n",
    "                # Include users mentioned in retweet\n",
    "                if use_rt_mentions:\n",
    "                    incl_users.union(tweet['user_mentions'])\n",
    "            # Include mentioned users (non-retweet case)\n",
    "            elif use_mentions:\n",
    "                incl_users.union(tweet['user_mentions'])\n",
    "            \n",
    "            # Add to users' context tweets\n",
    "            for u in incl_users:\n",
    "                if u in self.user_ct_tweets:\n",
    "                    self.user_ct_tweets[u].append(tweet)\n",
    "                else:\n",
    "                    self.user_ct_tweets[u] = [tweet]\n",
    "        \n",
    "        # Sort context tweets chronologically\n",
    "        for u in self.user_ct_tweets:\n",
    "            self.user_ct_tweets[u] = sorted(self.user_ct_tweets[u], key=lambda t: t['created_at'])\n",
    "    \n",
    "    \n",
    "    def get_tweet_embedding(self, tweet_id):\n",
    "        '''\n",
    "        Get the tweet embedding for the given tweet.\n",
    "        Args:\n",
    "            tweet_id (int): the id of the tweet, according to twitter's ID system\n",
    "        Returns:\n",
    "            the tweet embedding\n",
    "        '''\n",
    "        return np.zeros(self.embeddings_dim, )\n",
    "    \n",
    "    \n",
    "    def create_context_embedding(self, user_id, tweet_idx):\n",
    "        '''\n",
    "        Get the context embedding for the given tweet, determined by user and index.\n",
    "        Args:\n",
    "            user_id (int): the id of the user, according to data_loader's user ids\n",
    "            tweet_idx (int): the index of the tweet in self.user_ct_tweets[user_id]\n",
    "        '''\n",
    "        # Return difference in days, as a float\n",
    "        def days_diff(d1, d2):\n",
    "            return (d1 - d2).seconds/60/60/24\n",
    "        \n",
    "        tweet_embs = []\n",
    "        \n",
    "        today = self.user_ct_tweets[user_id][tweet_idx]['created_at']\n",
    "        i = tweet_idx-1\n",
    "        while i >= 0 and days_diff(today, self.user_ct_tweets[user_id][i]['created_at']) \\\n",
    "                                     < self.context_size:\n",
    "            # Get embedding -- may need to change\n",
    "            emb = self.get_tweet_embedding(self.user_ct_tweets[user_id][i]['tweet_id'])\n",
    "            # Weigh embedding\n",
    "            diff = days_diff(today, self.user_ct_tweets[user_id][i]['created_at'])\n",
    "            weight = self.decay_rate ** (diff/self.context_hl)\n",
    "            emb = emb * weight\n",
    "            # Save\n",
    "            tweet_embs.append(emb)\n",
    "            i -= 1\n",
    "        \n",
    "        result = None\n",
    "        if len(tweet_embs) == 0:\n",
    "            result = np.zeros(self.embeddings_dim, )\n",
    "        else:\n",
    "            if self.context_combine == 'avg':\n",
    "                result = np.mean(np.array(tweet_embs), axis=0)\n",
    "            elif self.context_combine == 'sum':\n",
    "                result = sum(tweet_embs)\n",
    "            elif self.context_combine == 'max':\n",
    "                result = np.max(np.array(tweet_embs), axis=0)\n",
    "            else:\n",
    "                raise ValueError('Unknown settting for context_combine:', context_combine)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def create_context_embeddings(self):\n",
    "        '''\n",
    "        Create the context embeddings for the tweets.\n",
    "        '''\n",
    "        self.tweet_to_ct = {} # Reset embeddings\n",
    "        # For now, go through all tweets and only pick out the labled ones\n",
    "        for u in self.user_ct_tweets:\n",
    "            for idx, t in enumerate(self.user_ct_tweets[u]):\n",
    "                if 'label' in t:\n",
    "                    # Save the context embedding\n",
    "                    self.tweet_to_ct[t['tweet_id']] = self.create_context_embedding(t['user_post'], idx)\n",
    "            print(u, 'done')\n",
    "    \n",
    "    \n",
    "    def get_context_embedding(self, tweet_id):\n",
    "        '''\n",
    "        Get the context embedding for the specified tweet, determined by tweet_id\n",
    "        Args:\n",
    "            tweet_id (int): the id of the tweet, according to the twitter tweet ids\n",
    "        Returns:\n",
    "            (np.array(int)): the context embedding \n",
    "        '''\n",
    "        if len(self.tweet_to_ct) == 0:\n",
    "            raise ValueError('Context embeddings have not been created yet. Call create_context_embeddings().')\n",
    "        if tweet_id not in self.tweet_to_ct:\n",
    "            raise valueError('No calcualted context embedding for given tweet_id:', tweet_id)\n",
    "        \n",
    "        return self.tweet_to_ct[tweet_id]\n",
    "\n",
    "    \n",
    "    def write_context_embeddings(self, out_file=None):\n",
    "        '''\n",
    "        Writes the embeddings to a file.\n",
    "        Args:\n",
    "            out_file (str): the path of the file to write to\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        if not out_file:\n",
    "            out_file = 'context_emb_{0}_{1}_rt{2}_men{3}_rtmen{4}_hl{5}.csv' \\\n",
    "                        .format(self.context_size, self.context_combine, self.use_rt_user, \n",
    "                                self.use_mentions, self.use_rt_mentions, self.context_hl)\n",
    "        with open(out_file, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(['tweet_id', 'context_embedding'])\n",
    "            for tweet_id, ct_emb in self.tweet_to_ct.items():\n",
    "                writer.writerow([tweet_id, ct_emb])\n",
    "                \n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester/usage\n",
    "\n",
    "# number of days\n",
    "context_size = 5 \n",
    "\n",
    "# method of combining tweet embeddings\n",
    "context_combine = 'avg' \n",
    "\n",
    "# User A retweets User B's tweet -- if true: the retweet will be counted in both User A and User B's context\n",
    "use_rt_user = False\n",
    "\n",
    "# User A tweets, mentioning User B -- if true: this tweet will be in User A and User B's context\n",
    "use_mentions = True\n",
    "\n",
    "# User A retweets User B's tweet, which originally mentioned User C -- if true: counted in A, B and C's history\n",
    "use_rt_mentions = False\n",
    "\n",
    "# the data loader\n",
    "data_loader = dl\n",
    "\n",
    "# will eventually take params, but for now all we need is the data loader\n",
    "contextifier = Contextifier(context_size, context_combine, use_rt_user, use_mentions, use_rt_mentions, context_hl=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextifier.create_user_context_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 done\n",
      "59 done\n",
      "245 done\n",
      "15 done\n",
      "93 done\n",
      "42 done\n",
      "158 done\n",
      "2984 done\n",
      "125 done\n",
      "41 done\n",
      "167 done\n",
      "243 done\n",
      "157 done\n",
      "75 done\n",
      "50 done\n",
      "45 done\n",
      "214 done\n",
      "378 done\n",
      "3001 done\n",
      "100 done\n",
      "194 done\n",
      "2995 done\n",
      "111 done\n",
      "196 done\n",
      "12 done\n",
      "188 done\n",
      "252 done\n",
      "218 done\n",
      "273 done\n",
      "200 done\n",
      "148 done\n",
      "162 done\n",
      "49 done\n",
      "5 done\n",
      "124 done\n",
      "262 done\n",
      "549 done\n",
      "3013 done\n",
      "204 done\n",
      "147 done\n",
      "23 done\n",
      "295 done\n",
      "71 done\n",
      "265 done\n",
      "77 done\n",
      "61 done\n",
      "189 done\n",
      "420 done\n",
      "2999 done\n",
      "90 done\n",
      "253 done\n",
      "2989 done\n",
      "215 done\n",
      "83 done\n",
      "206 done\n",
      "244 done\n",
      "36 done\n",
      "184 done\n",
      "81 done\n",
      "170 done\n",
      "213 done\n",
      "104 done\n",
      "137 done\n",
      "8 done\n",
      "187 done\n",
      "57 done\n",
      "4 done\n",
      "17 done\n",
      "112 done\n",
      "727 done\n",
      "208 done\n",
      "220 done\n",
      "209 done\n",
      "55 done\n",
      "1147 done\n",
      "379 done\n",
      "171 done\n",
      "146 done\n",
      "64 done\n",
      "2985 done\n",
      "203 done\n",
      "357 done\n",
      "72 done\n",
      "47 done\n",
      "117 done\n",
      "3003 done\n",
      "195 done\n",
      "150 done\n",
      "20 done\n",
      "31 done\n",
      "25 done\n",
      "145 done\n",
      "149 done\n",
      "30 done\n",
      "116 done\n",
      "52 done\n",
      "179 done\n",
      "177 done\n",
      "2998 done\n",
      "256 done\n",
      "3012 done\n",
      "216 done\n",
      "231 done\n",
      "2996 done\n",
      "726 done\n",
      "21 done\n",
      "2988 done\n",
      "85 done\n",
      "207 done\n",
      "236 done\n",
      "19 done\n",
      "96 done\n",
      "68 done\n",
      "190 done\n",
      "3 done\n",
      "66 done\n",
      "176 done\n",
      "1146 done\n",
      "87 done\n",
      "222 done\n",
      "70 done\n",
      "106 done\n",
      "103 done\n",
      "259 done\n",
      "74 done\n",
      "56 done\n",
      "257 done\n",
      "40 done\n",
      "129 done\n",
      "141 done\n",
      "13 done\n",
      "153 done\n",
      "235 done\n",
      "164 done\n",
      "22 done\n",
      "246 done\n",
      "63 done\n",
      "248 done\n",
      "9 done\n",
      "178 done\n",
      "122 done\n",
      "132 done\n",
      "39 done\n",
      "225 done\n",
      "286 done\n",
      "238 done\n",
      "114 done\n",
      "82 done\n",
      "217 done\n",
      "29 done\n",
      "233 done\n",
      "123 done\n",
      "53 done\n",
      "263 done\n",
      "163 done\n",
      "159 done\n",
      "219 done\n",
      "165 done\n",
      "254 done\n",
      "73 done\n",
      "223 done\n",
      "37 done\n",
      "65 done\n",
      "300 done\n",
      "18 done\n",
      "2990 done\n",
      "110 done\n",
      "143 done\n",
      "3000 done\n",
      "175 done\n",
      "43 done\n",
      "1148 done\n",
      "67 done\n",
      "88 done\n",
      "2987 done\n",
      "2994 done\n",
      "26 done\n",
      "97 done\n",
      "127 done\n",
      "169 done\n",
      "239 done\n",
      "105 done\n",
      "32 done\n",
      "166 done\n",
      "3002 done\n",
      "38 done\n",
      "80 done\n",
      "3008 done\n",
      "174 done\n",
      "227 done\n",
      "224 done\n",
      "266 done\n",
      "1150 done\n",
      "250 done\n",
      "2981 done\n",
      "199 done\n",
      "126 done\n",
      "92 done\n",
      "251 done\n",
      "3009 done\n",
      "89 done\n",
      "120 done\n",
      "86 done\n",
      "2992 done\n",
      "138 done\n",
      "197 done\n",
      "211 done\n",
      "247 done\n",
      "221 done\n",
      "115 done\n",
      "212 done\n",
      "79 done\n",
      "183 done\n",
      "1168 done\n",
      "76 done\n",
      "48 done\n",
      "16 done\n",
      "107 done\n",
      "267 done\n",
      "131 done\n",
      "58 done\n",
      "46 done\n",
      "260 done\n",
      "234 done\n",
      "3005 done\n",
      "151 done\n",
      "279 done\n",
      "133 done\n",
      "44 done\n",
      "2986 done\n",
      "172 done\n",
      "205 done\n",
      "152 done\n",
      "198 done\n",
      "292 done\n",
      "185 done\n",
      "60 done\n",
      "237 done\n",
      "101 done\n",
      "35 done\n",
      "102 done\n",
      "84 done\n",
      "28 done\n",
      "130 done\n",
      "193 done\n",
      "62 done\n",
      "3004 done\n",
      "6 done\n",
      "2983 done\n",
      "728 done\n",
      "2991 done\n",
      "358 done\n",
      "422 done\n",
      "161 done\n",
      "168 done\n",
      "173 done\n",
      "191 done\n",
      "118 done\n",
      "210 done\n",
      "14 done\n",
      "3007 done\n",
      "78 done\n",
      "160 done\n",
      "2982 done\n",
      "229 done\n",
      "140 done\n",
      "339 done\n",
      "51 done\n",
      "228 done\n",
      "3010 done\n",
      "91 done\n",
      "139 done\n",
      "2993 done\n",
      "3006 done\n",
      "156 done\n",
      "421 done\n",
      "3029 done\n",
      "192 done\n",
      "2997 done\n",
      "154 done\n",
      "99 done\n",
      "94 done\n",
      "261 done\n",
      "2 done\n",
      "2964 done\n",
      "355 done\n",
      "1128 done\n",
      "128 done\n",
      "135 done\n",
      "323 done\n",
      "142 done\n",
      "155 done\n",
      "134 done\n",
      "98 done\n",
      "186 done\n",
      "3283 done\n",
      "285 done\n",
      "113 done\n",
      "547 done\n",
      "3017 done\n",
      "3058 done\n",
      "3402 done\n",
      "1149 done\n",
      "730 done\n",
      "10 done\n",
      "706 done\n",
      "24 done\n",
      "3098 done\n",
      "1 done\n",
      "718 done\n",
      "345 done\n",
      "272 done\n",
      "1389 done\n",
      "425 done\n",
      "6059 done\n",
      "240 done\n",
      "281 done\n",
      "181 done\n",
      "3576 done\n",
      "7 done\n",
      "54 done\n",
      "3081 done\n",
      "3061 done\n",
      "136 done\n",
      "7160 done\n",
      "34 done\n",
      "33 done\n",
      "304 done\n",
      "376 done\n",
      "182 done\n",
      "69 done\n",
      "1121 done\n",
      "144 done\n",
      "242 done\n",
      "528 done\n",
      "109 done\n",
      "2730 done\n",
      "344 done\n",
      "249 done\n",
      "269 done\n",
      "271 done\n",
      "255 done\n",
      "230 done\n",
      "1097 done\n",
      "11 done\n",
      "720 done\n",
      "543 done\n",
      "108 done\n",
      "6700 done\n",
      "455 done\n",
      "276 done\n",
      "1749 done\n",
      "278 done\n",
      "241 done\n",
      "277 done\n",
      "1608 done\n",
      "258 done\n",
      "1136 done\n",
      "414 done\n",
      "839 done\n",
      "563 done\n",
      "1137 done\n",
      "299 done\n",
      "282 done\n",
      "232 done\n",
      "290 done\n",
      "294 done\n",
      "469 done\n",
      "202 done\n",
      "1018 done\n",
      "121 done\n",
      "1129 done\n",
      "713 done\n",
      "1111 done\n",
      "6708 done\n",
      "1144 done\n",
      "1069 done\n",
      "1108 done\n",
      "2820 done\n",
      "348 done\n",
      "316 done\n",
      "309 done\n",
      "1393 done\n",
      "356 done\n",
      "418 done\n",
      "1112 done\n",
      "95 done\n",
      "1116 done\n",
      "354 done\n",
      "1122 done\n",
      "1519 done\n",
      "700 done\n",
      "413 done\n",
      "2257 done\n",
      "375 done\n",
      "1124 done\n",
      "545 done\n",
      "330 done\n",
      "1856 done\n",
      "631 done\n",
      "882 done\n",
      "2431 done\n",
      "2358 done\n",
      "716 done\n",
      "1748 done\n",
      "2663 done\n",
      "1090 done\n",
      "1114 done\n",
      "1125 done\n",
      "468 done\n",
      "548 done\n",
      "703 done\n",
      "1140 done\n",
      "2380 done\n",
      "1091 done\n",
      "2903 done\n",
      "2200 done\n",
      "3389 done\n",
      "2587 done\n",
      "1900 done\n",
      "2880 done\n",
      "402 done\n",
      "1101 done\n",
      "2605 done\n",
      "1145 done\n",
      "2684 done\n",
      "1123 done\n",
      "415 done\n",
      "1135 done\n",
      "719 done\n",
      "2204 done\n",
      "1769 done\n",
      "2640 done\n",
      "1093 done\n",
      "2901 done\n",
      "1587 done\n",
      "664 done\n",
      "2636 done\n",
      "1049 done\n",
      "409 done\n",
      "399 done\n",
      "270 done\n",
      "1175 done\n",
      "2961 done\n",
      "1142 done\n",
      "691 done\n",
      "593 done\n",
      "1199 done\n",
      "525 done\n",
      "2704 done\n",
      "453 done\n",
      "541 done\n",
      "657 done\n",
      "320 done\n",
      "2942 done\n",
      "1126 done\n",
      "2523 done\n",
      "1107 done\n",
      "1712 done\n",
      "1131 done\n",
      "2323 done\n",
      "2934 done\n",
      "2557 done\n",
      "1554 done\n",
      "950 done\n",
      "2351 done\n",
      "2628 done\n",
      "2804 done\n",
      "2832 done\n",
      "2544 done\n",
      "933 done\n",
      "1202 done\n",
      "2718 done\n",
      "2303 done\n",
      "2205 done\n",
      "2589 done\n",
      "682 done\n",
      "2517 done\n",
      "2409 done\n",
      "1139 done\n"
     ]
    }
   ],
   "source": [
    "contextifier.create_context_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    }
   ],
   "source": [
    "print(len(contextifier.user_ct_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contextifier.get_context_embedding(631443130563330048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextifier.write_context_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
