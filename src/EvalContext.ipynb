{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.svm\n",
    "import sklearn.metrics as skm\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from data_loader import Data_loader\n",
    "from represent_tweet_level import TweetLevel\n",
    "\n",
    "class Contextifier:\n",
    "    '''\n",
    "        Creates the context for tweets.\n",
    "    '''\n",
    "    # Magic strings to determine relationship between user and tweet\n",
    "    SELF = 'SELF'\n",
    "    RETWEET = 'RETWEET'\n",
    "    MENTION = 'MENTION'\n",
    "    RETWEET_MENTION = 'RETWEET_MENTION'\n",
    "    \n",
    "    \n",
    "    def __init__(self, data_loader, context_size=1, use_rt_user=False, \n",
    "                 use_mentions=False, use_rt_mentions=False, context_hl_ratio=1.0,\n",
    "                 word_emb_file='../data/w2v_word_s300_w5_mc5_it20.bin',\n",
    "                 word_emb_type='w2v',\n",
    "                 word_emb_mode='avg',\n",
    "                 use_word_ct=True,\n",
    "                 splex_emb_file='../data/splex_standard_svd_word_s300_seeds_hc.pkl',\n",
    "                 splex_emb_mode='sum',\n",
    "                 use_splex_ct=True,\n",
    "                 keep_stats=False\n",
    "                 ):\n",
    "        '''\n",
    "        Create it!\n",
    "        Args:\n",
    "            data_loader (Data_loader): an instance of the Data_loader class, from which to obtain data.\n",
    "            context_size (float): Number of days to look back\n",
    "            use_rt_user (bool): User A retweets User B's tweet -- if true,\n",
    "                    this tweet will be counted in User A and User B's context\n",
    "            use_mentions (bool): User A tweets, mentioning User B -- if true, \n",
    "                    this tweet will be in User A and User B's context\n",
    "            use_rt_mentions (bool): User A retweets User B's tweet, which mentioned User C -- if true,\n",
    "                    this tweet will counted in User A and User C's history\n",
    "            context_hl_ratio (float): Ratio of half life to context size. Tweet embeddings will be weighed according to\n",
    "                    (self.decay_rate)^(t/x) where t is the number of days the previous tweet is \n",
    "                    from the current one, and x is context_size * context_hl Set to 0 for no weighting/decay.\n",
    "            word_emb_file (str): the path to the file to saved word embeddings\n",
    "            word_emb_file (str): the type of the word embedding file, e.g. 'w2v'. See TweetLevel for more info.\n",
    "            word_emb_mode (str): the mode to use when combining word embeddings at TweetLevel, e.g. 'avg'\n",
    "            use_word_ct (bool): if true, word embeddings of --context-- will be used\n",
    "            splex_emb_file (str): the pickle file that contains the splex embeddings.\n",
    "            splex_emb_mode (str): the mode to use when combining splex scores at TweetLevel, e.g. 'sum'\n",
    "            use_splex_ct (bool): if true, splex embeddings of --context-- will be used\n",
    "            keep_stats (bool): if true, keep stats (e.g. what tweets are in a context window)\n",
    "        '''\n",
    "        # Save variables\n",
    "        self.set_context_size(context_size)\n",
    "        self.use_rt_user = None\n",
    "        self.use_mentions = None\n",
    "        self.use_rt_mentions = None\n",
    "        self.set_use_rt_user(use_rt_user)\n",
    "        self.set_use_mentions(use_mentions)\n",
    "        self.set_use_rt_mentions(use_rt_mentions)\n",
    "        self.set_context_hl_ratio(context_hl_ratio)\n",
    "        \n",
    "        \n",
    "        # Load data\n",
    "        self.all_data = data_loader.all_data()\n",
    "        \n",
    "        \n",
    "        # Tweet to context embedding cache\n",
    "        self.tweet_to_ct = {}\n",
    "        \n",
    "        # Initializing tools to get tweet-level embeddings\n",
    "        self.word_emb_file = None\n",
    "        self.splex_emb_file = None\n",
    "        self.set_embeddings(word_emb_file, word_emb_type, word_emb_mode, use_word_ct, \n",
    "                            splex_emb_file, splex_emb_mode, use_splex_ct)\n",
    "        \n",
    "\n",
    "        # Keeping stats\n",
    "        self.set_keep_stats(keep_stats)\n",
    "\n",
    "            \n",
    "    def set_context_size(self, context_size):\n",
    "        self.context_size = context_size\n",
    "        self.reset_context_embeddings()\n",
    "        self.reset_stats()\n",
    "        \n",
    "    def set_use_rt_user(self, use_rt_user):\n",
    "        self.use_rt_user = use_rt_user\n",
    "        self._set_post_types() # update post types\n",
    "    \n",
    "    def set_use_mentions(self, use_mentions):\n",
    "        self.use_mentions = use_mentions\n",
    "        self._set_post_types() # update post types\n",
    "    \n",
    "    def set_use_rt_mentions(self, use_rt_mentions):\n",
    "        self.use_rt_mentions = use_rt_mentions\n",
    "        self._set_post_types() # update post types\n",
    "\n",
    "    \n",
    "    def set_context_hl_ratio(self, context_hl_ratio):\n",
    "        self.context_hl_ratio = context_hl_ratio\n",
    "        self.decay_rate = 0.5 # hardcoded!\n",
    "        self.reset_context_embeddings()\n",
    "        \n",
    "    def set_embeddings(self, word_emb_file, word_emb_type, word_emb_mode, use_word_ct,\n",
    "                       splex_emb_file, splex_emb_mode, use_splex_ct):\n",
    "        # Cache for combined tweet embeddings\n",
    "        self.tweet_emb_cache = {}\n",
    "        \n",
    "        self.use_word_ct = use_word_ct\n",
    "        # Initializing tools to get tweet-level embeddings\n",
    "        if self.use_word_ct and self.word_emb_file != word_emb_file: # don't reload if hasn't changed\n",
    "            self.word_emb_file = word_emb_file\n",
    "            self.tl_word = TweetLevel(word_level=word_emb_file, wl_file_type=word_emb_type)\n",
    "        self.word_emb_mode = word_emb_mode\n",
    "        # Cache for calculated tweet-level word embeddings\n",
    "        self.tweet_word_cache = {}\n",
    "        \n",
    "        self.use_splex_ct = use_splex_ct\n",
    "        if self.use_splex_ct and self.splex_emb_file != splex_emb_file: # don't reload if hasn't changed\n",
    "            self.splex_emb_file = splex_emb_file\n",
    "            self.tl_splex = TweetLevel(word_level=splex_emb_file, wl_file_type='pkl')\n",
    "        self.splex_emb_mode = splex_emb_mode\n",
    "        # Cache for calculated tweet-level splex embeddings\n",
    "        self.tweet_splex_cache = {}\n",
    "        \n",
    "        # Hardcoding embedding size -- unsure how to change this\n",
    "        self.embeddings_dim = 300 + 3\n",
    "        \n",
    "        self.reset_context_embeddings()\n",
    "    \n",
    "        \n",
    "    def set_keep_stats(self, keep_stats):\n",
    "        self.keep_stats = keep_stats\n",
    "        if self.keep_stats:\n",
    "            # Tweet id to tweet ids in context window\n",
    "            self.tweet_to_ct_tweets = {}\n",
    "        self.reset_context_embeddings()\n",
    "            \n",
    "            \n",
    "    def _set_post_types(self):\n",
    "        # Update post types to use in context\n",
    "        self.post_types = set()\n",
    "        self.post_types.add(self.SELF) # Always include self posts\n",
    "        if self.use_rt_user:\n",
    "            self.post_types.add(self.RETWEET)\n",
    "        if self.use_rt_mentions:\n",
    "            self.post_types.add(self.RETWEET_MENTION)\n",
    "        if self.use_mentions:\n",
    "            self.post_types.add(self.MENTION)\n",
    "        self.reset_context_embeddings() # Reset cache\n",
    "        self.reset_stats() # Reset stats\n",
    "        \n",
    "    \n",
    "    def create_user_context_tweets(self):\n",
    "        '''\n",
    "        Sorts the tweets into self.user_ct_tweets, based on the variables\n",
    "            self.use_rt_user, self.use_rt_mentions, and self.use_mentions\n",
    "        '''\n",
    "        # Tweets in a user's \"context\"\n",
    "        self.user_ct_tweets = {}\n",
    "        \n",
    "        # Map from tweet id to tuple of (user, idx in sorted list)\n",
    "        # Note that \"user\" is user_post, the user who posted the tweet\n",
    "        self.id_to_location = {}\n",
    "        \n",
    "        # For every tweet in the dataset (labled and unlabeled)\n",
    "        for tweet in self.all_data:\n",
    "            incl_users = []\n",
    "            # Always include poster\n",
    "            incl_users.append((tweet['user_post'], self.SELF))\n",
    "            # Check if tweet is a retweet\n",
    "            if 'user_retweet' in tweet:\n",
    "                incl_users.append((tweet['user_retweet'], self.RETWEET))\n",
    "                # Include users mentioned in retweet\n",
    "                rt_mentions = [(u, self.RETWEET_MENTION) for u in tweet['user_mentions']]\n",
    "                incl_users.extend(rt_mentions)\n",
    "            else:\n",
    "                # Include users mentioned (not retweet)\n",
    "                mentions = [(u, self.MENTION) for u in tweet['user_mentions']]\n",
    "                incl_users.extend(mentions)\n",
    "            \n",
    "            # Add tweets to users' context tweets\n",
    "            for u, post_type in incl_users:\n",
    "                if u in self.user_ct_tweets:\n",
    "                    self.user_ct_tweets[u].append((tweet, post_type))\n",
    "                else:\n",
    "                    self.user_ct_tweets[u] = [(tweet, post_type)]\n",
    "        \n",
    "        # Sort context tweets chronologically\n",
    "        for u in self.user_ct_tweets:\n",
    "            self.user_ct_tweets[u] = sorted(self.user_ct_tweets[u], key=lambda t: t[0]['created_at'])\n",
    "            \n",
    "        # Go through the tweets to save their location\n",
    "        for u, tweets in self.user_ct_tweets.items():\n",
    "            for idx, t in enumerate(tweets):\n",
    "                if u == t[0]['user_post']:\n",
    "                    self.id_to_location[t[0]['tweet_id']] = (u, idx)\n",
    "    \n",
    "    \n",
    "    def get_tweet_embedding(self, tweet_id):\n",
    "        '''\n",
    "        Get the tweet embedding for the given tweet.\n",
    "        Args:\n",
    "            tweet_id (int): the id of the tweet, according to twitter's ID system\n",
    "        Returns:\n",
    "            the tweet embedding\n",
    "        '''\n",
    "        if tweet_id in self.tweet_emb_cache: # Check cache for embedding\n",
    "            return self.tweet_emb_cache[tweet_id]\n",
    "        else:\n",
    "            w_emb = self.tl_word.get_representation(tweet_id, mode=self.word_emb_mode)\n",
    "            sp_emb =  self.tl_splex.get_representation(tweet_id, mode=self.splex_emb_mode)\n",
    "            full_emb = np.concatenate([w_emb, sp_emb])\n",
    "            self.tweet_emb_cache[tweet_id] = full_emb # Save embedding to cache\n",
    "            return full_emb\n",
    "\n",
    "        \n",
    "    def get_word_embedding(self, tweet_id):\n",
    "        # add cache back in here\n",
    "        if tweet_id in self.tweet_word_cache:\n",
    "            return self.tweet_word_cache[tweet_id]\n",
    "        else:\n",
    "            res = self.tl_word.get_representation(tweet_id, mode=self.word_emb_mode)\n",
    "            self.tweet_word_cache[tweet_id] = res\n",
    "            return res\n",
    "\n",
    "        \n",
    "    def get_splex_embedding(self, tweet_id):\n",
    "        # add cache back in here\n",
    "        if tweet_id in self.tweet_splex_cache:\n",
    "            return self.tweet_splex_cache[tweet_id]\n",
    "        else:\n",
    "            res = self.tl_splex.get_representation(tweet_id, mode=self.splex_emb_mode)\n",
    "            self.tweet_splex_cache[tweet_id] = res\n",
    "            return res\n",
    "\n",
    "\n",
    "    def combine_embeddings(self, embeddings, mode):\n",
    "        # documentation\n",
    "        result = None\n",
    "        if mode == 'avg':\n",
    "            result = np.mean(np.array(embeddings), axis=0)\n",
    "        elif mode == 'sum':\n",
    "                result = sum(embeddings)\n",
    "        elif mode == 'max':\n",
    "            result = np.max(np.array(embeddings), axis=0)\n",
    "        else:\n",
    "            raise ValueError('Unknown combination method:', mode)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    def create_context_embedding(self, user_id, tweet_idx):\n",
    "        '''\n",
    "        Get the context embedding for the given tweet, determined by user and index.\n",
    "        Args:\n",
    "            user_id (int): the id of the user, according to data_loader's user ids\n",
    "            tweet_idx (int): the index of the tweet in self.user_ct_tweets[user_id]\n",
    "        '''\n",
    "        # Check if context embedding is in the cache\n",
    "        tweet_id = self.user_ct_tweets[user_id][tweet_idx][0]['tweet_id']\n",
    "        if tweet_id in self.tweet_to_ct:\n",
    "            return self.tweet_to_ct[tweet_id]\n",
    "        \n",
    "        # Return difference in days, as a float\n",
    "        def days_diff(d1, d2):\n",
    "            return (d1 - d2).total_seconds() / 60 / 60 / 24\n",
    "        \n",
    "        w_embs = [] # word embeddings\n",
    "        splex_embs = [] # splex embeddings\n",
    "        tweet_ids = [] # for stats\n",
    "        context_hl = self.context_size * self.context_hl_ratio # set half life\n",
    "        \n",
    "        today = self.user_ct_tweets[user_id][tweet_idx][0]['created_at']\n",
    "        i = tweet_idx-1\n",
    "        while i >= 0 and days_diff(today, self.user_ct_tweets[user_id][i][0]['created_at']) \\\n",
    "                                     < self.context_size:\n",
    "            \n",
    "            # Confirm post type is one we want to include\n",
    "            post_type = self.user_ct_tweets[user_id][i][1]\n",
    "            if post_type not in self.post_types:\n",
    "                i -= 1\n",
    "                continue \n",
    "            \n",
    "            # Save tweet ids\n",
    "            if self.keep_stats:\n",
    "                tweet_ids.append(self.user_ct_tweets[user_id][i][0]['tweet_id'])\n",
    "\n",
    "            # Get embeddings -- may need to change\n",
    "            w_emb = self.get_word_embedding(self.user_ct_tweets[user_id][i][0]['tweet_id'])\n",
    "            splex_emb = self.get_splex_embedding(self.user_ct_tweets[user_id][i][0]['tweet_id'])\n",
    "\n",
    "            # Weigh embedding\n",
    "            if context_hl != 0:\n",
    "                diff = days_diff(today, self.user_ct_tweets[user_id][i][0]['created_at'])\n",
    "                weight = self.decay_rate ** (diff/context_hl)\n",
    "                w_emb = w_emb * weight\n",
    "                splex_emb = splex_emb * weight\n",
    "\n",
    "            # Save\n",
    "            w_embs.append(w_emb)\n",
    "            splex_embs.append(splex_emb)\n",
    "            i -= 1\n",
    "\n",
    "        # Save stats\n",
    "        if self.keep_stats:\n",
    "            self.tweet_to_ct_tweets[tweet_id] = tweet_ids\n",
    "        \n",
    "        # Combine word embeddings\n",
    "        w_comb = None\n",
    "        if len(w_embs) == 0:\n",
    "            w_comb = np.zeros(300, ) #AH! i don't have to hardcode these now\n",
    "        else:\n",
    "            w_comb = self.combine_embeddings(w_embs, self.word_emb_mode)\n",
    "\n",
    "        # Combine splex embeddings\n",
    "        splex_comb = None\n",
    "        if len(splex_embs) == 0:\n",
    "            splex_comb = np.zeros(3, ) # still hardcoded\n",
    "        else:\n",
    "            splex_comb = self.combine_embeddings(splex_embs, self.splex_emb_mode)    \n",
    "        \n",
    "        # Check if we're using word embeddings and splex embeddings\n",
    "        to_use = []\n",
    "        if self.use_word_ct:\n",
    "            to_use.append(w_comb)\n",
    "        if self.use_splex_ct:\n",
    "            to_use.append(splex_comb)\n",
    "            \n",
    "        # Concatenate to get result\n",
    "        result = np.concatenate(to_use)\n",
    "\n",
    "        # Cache the result\n",
    "        self.tweet_to_ct[tweet_id] = result\n",
    "        return result\n",
    "\n",
    "    def reset_context_embeddings(self):\n",
    "        self.tweet_to_ct = {} # Reset embeddings\n",
    "    \n",
    "    def reset_stats(self):\n",
    "        self.tweet_to_ct_tweets = {} # Reset stats\n",
    "    \n",
    "    \n",
    "    def create_context_embeddings(self):\n",
    "        '''\n",
    "        Create the context embeddings for the tweets.\n",
    "        '''\n",
    "        for fold_idx in range(0, 5):\n",
    "            tr, val, test = self.dl.cv_data(fold_idx)\n",
    "            all_tweets = [t for l in [tr, val, test] for t in l ]\n",
    "            for tweet in all_tweets: \n",
    "                self.tweet_to_ct[tweet['tweet_id']] = self.create_context_embedding(\n",
    "                    *self.id_to_location[tweet['tweet_id']])\n",
    "    \n",
    "    \n",
    "    def get_context_embedding(self, tweet_id):\n",
    "        '''\n",
    "        Get the context embedding for the specified tweet, determined by tweet_id\n",
    "        Args:\n",
    "            tweet_id (int): the id of the tweet, according to the twitter tweet ids\n",
    "        Returns:\n",
    "            (np.array(int)): the context embedding \n",
    "        '''\n",
    "        if len(self.user_ct_tweets) == 0:\n",
    "            raise ValueError('User contexts have not been created. First call .create_user_context_tweets().')\n",
    "        if tweet_id in self.tweet_to_ct:\n",
    "            return self.tweet_to_ct[tweet_id]\n",
    "        else:\n",
    "            # note: some weirdness going on here with loading from files\n",
    "            return self.create_context_embedding(*self.id_to_location[tweet_id])\n",
    "\n",
    "\n",
    "    def get_context_tweets(self, tweet_id):\n",
    "        # return ids of tweets in context\n",
    "        if tweet_id in self.tweet_to_ct_tweets:\n",
    "            return self.tweet_to_ct_tweets[tweet_id]\n",
    "        else:\n",
    "            raise ValueError('no calculated tweet ids in context') # fix this\n",
    "\n",
    "    \n",
    "    def from_file(self, in_file):\n",
    "        '''\n",
    "        Reads the context embeddings in from a file.\n",
    "        Args:\n",
    "            in_file (str): the path to the file, in csv format, <tweet_id>, <embedding>\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        with open(in_file, newline='') as csvfile:\n",
    "            reader = csv.DictReader(csvfile)\n",
    "            for row in reader:\n",
    "                self.tweet_to_ct[int(row['tweet_id'])] = np.fromstring(row['context_embedding'],\n",
    "                                                                    dtype=float, sep=' ')\n",
    "        \n",
    "\n",
    "    \n",
    "    def write_context_embeddings(self, out_file=None):\n",
    "        '''\n",
    "        Writes the embeddings to a file.\n",
    "        Args:\n",
    "            out_file (str): the path of the file to write to\n",
    "        Returns:\n",
    "            None\n",
    "        '''\n",
    "        if not out_file:\n",
    "            out_file = 'context_emb_{0}_{1}_rt{2}_men{3}_rtmen{4}_hlr{5}_.csv' \\\n",
    "                        .format(self.context_size, self.context_combine, self.use_rt_user, \n",
    "                                self.use_mentions, self.use_rt_mentions, self.context_hl_ratio)\n",
    "        with open(out_file, 'w', newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=',')\n",
    "            writer.writerow(['tweet_id', 'context_embedding'])\n",
    "            for tweet_id, ct_emb in self.tweet_to_ct.items():\n",
    "                ct_emb_str = ' '.join([str(x) for x in ct_emb])\n",
    "                writer.writerow([tweet_id, ct_emb_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "from data_loader import Data_loader\n",
    "option = 'word'\n",
    "max_len = 53\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TweetLevel...\n",
      "Number of embeddings in ../data/w2v_word_s300_w5_mc5_it20.bin: 23417\n",
      "Sample tweet_dict item: (740043438788345856, [2, 254, 440, 192, 94, 57, 72, 77])\n",
      "Size of tweet_dict: 1033655\n",
      "Initializing TweetLevel...\n",
      "Number of embeddings in ../data/splex_standard_svd_word_s300_seeds_hc.pkl: 20000\n",
      "Sample tweet_dict item: (740043438788345856, [2, 254, 440, 192, 94, 57, 72, 77])\n",
      "Size of tweet_dict: 1033655\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "contextifier = Contextifier(dl)\n",
    "# Create the contexts (only needs to be done once)\n",
    "contextifier.create_user_context_tweets()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg F-score: 0.49028847364223027\n",
      "Avg number of context tweets in window: 2766.3819178780923\n",
      "{'context_hl_ratio': 0, 'context_size': 1000, 'keep_stats': True, 'splex_emb_file': '../data/splex_minmax_svd_word_s300_seeds_hc.pkl', 'splex_emb_mode': 'sum', 'use_mentions': False, 'use_rt_mentions': False, 'use_rt_user': False, 'use_splex_ct': True, 'use_word_ct': False, 'word_emb_file': '../data/w2v_word_s300_w5_mc5_it20.bin', 'word_emb_mode': 'avg', 'word_emb_type': 'w2v'}\n",
      "BEST F: 0.49028847364223027\n",
      "BEST CONTEXT: 2766.3819178780923\n",
      "BEST PARAMS: {'context_hl_ratio': 0, 'context_size': 1000, 'keep_stats': True, 'splex_emb_file': '../data/splex_minmax_svd_word_s300_seeds_hc.pkl', 'splex_emb_mode': 'sum', 'use_mentions': False, 'use_rt_mentions': False, 'use_rt_user': False, 'use_splex_ct': True, 'use_word_ct': False, 'word_emb_file': '../data/w2v_word_s300_w5_mc5_it20.bin', 'word_emb_mode': 'avg', 'word_emb_type': 'w2v'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# param_grid = {'context_size': [0.1, 0.25, 0.5, 2, 7, 14],\n",
    "#               'use_rt_user': [True, False],\n",
    "#               'use_mentions': [True, False],\n",
    "#               'use_rt_mentions': [True, False],\n",
    "#               'context_hl_ratio': [0, 0.1, 0.25, 0.5], # relative to size\n",
    "#               'word_emb_file': ['../data/w2v_word_s300_w5_mc5_it20.bin'],\n",
    "#               'word_emb_type': ['w2v'],\n",
    "#               'word_emb_mode': ['avg'],\n",
    "#               'use_word_ct': [False],\n",
    "#               'splex_emb_file': ['../data/splex_minmax_svd_word_s300_seeds_hc.pkl'],\n",
    "#               'splex_emb_mode':['sum'],\n",
    "#               'use_splex_ct': [True],\n",
    "#               'keep_stats': [True]\n",
    "#              }\n",
    "\n",
    "param_grid = {'context_size': [1000],\n",
    "              'use_rt_user': [False],\n",
    "              'use_mentions': [False],\n",
    "              'use_rt_mentions': [False],\n",
    "              'context_hl_ratio': [0], # relative to size\n",
    "              'word_emb_file': ['../data/w2v_word_s300_w5_mc5_it20.bin'],\n",
    "              'word_emb_type': ['w2v'],\n",
    "              'word_emb_mode': ['avg'],\n",
    "              'use_word_ct': [False],\n",
    "              'splex_emb_file': ['../data/splex_minmax_svd_word_s300_seeds_hc.pkl'],\n",
    "              'splex_emb_mode':['sum'],\n",
    "              'use_splex_ct': [True],\n",
    "              'keep_stats': [True]\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "best_f = 0\n",
    "best_params = None\n",
    "best_context = 0\n",
    "\n",
    "for params in grid:\n",
    "    contextifier.set_context_size(params['context_size'])\n",
    "    contextifier.set_use_rt_user(params['use_rt_user'])\n",
    "    contextifier.set_use_rt_mentions(params['use_rt_mentions'])\n",
    "    contextifier.set_context_hl_ratio(params['context_hl_ratio'])\n",
    "    contextifier.set_embeddings(params['word_emb_file'], \n",
    "                                params['word_emb_type'], \n",
    "                                params['word_emb_mode'],\n",
    "                                params['use_word_ct'],\n",
    "                                params['splex_emb_file'], \n",
    "                                params['splex_emb_mode'],\n",
    "                                params['use_splex_ct'])\n",
    "    contextifier.set_keep_stats(params['keep_stats'])\n",
    "    \n",
    "    total_f = 0\n",
    "    context_sizes = {}\n",
    "\n",
    "    class_weight = {\n",
    "        'Loss' : 0.35,\n",
    "        'Aggression': 0.5,\n",
    "        'Other': 0.15\n",
    "    }\n",
    "\n",
    "    for fold_idx in range(0, 5):\n",
    "    #     print('Fold:', fold_idx)\n",
    "        tr, val, test = dl.cv_data(fold_idx)\n",
    "\n",
    "        # Set up\n",
    "#         clf = sklearn.svm.LinearSVC() # no class weights\n",
    "        clf = sklearn.svm.LinearSVC(class_weight=class_weight) # with class weights\n",
    "#         vectorizer = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda s: s.split(' '))\n",
    "\n",
    "        # Training on both TR and VAL -- maybe a good idea?\n",
    "        all_train_tweets = [t for l in [tr, val] for t in l ]\n",
    "\n",
    "        # Train\n",
    "        train_ids = [t['tweet_id'] for t in all_train_tweets]\n",
    "#         train_texts = [' '.join([str(i) for i in t['int_arr']]) for t in all_train_tweets] # treat as texts of numbers\n",
    "#         X_train = vectorizer.fit_transform(train_texts)\n",
    "        y_train = [t['label'] for t in all_train_tweets]\n",
    "        tweet_embs, context_embs = [], []\n",
    "        for t_id in train_ids:\n",
    "                tweet_embs.append(contextifier.get_tweet_embedding(t_id))\n",
    "                context_embs.append(contextifier.get_context_embedding(t_id))\n",
    "                context_sizes[t_id] = len(contextifier.get_context_tweets(t_id)) #context size\n",
    "\n",
    "        X_train = hstack([csr_matrix(np.array(tweet_embs)), csr_matrix(np.array(context_embs))])\n",
    "#         X_train = hstack([csr_matrix(np.array(tweet_embs))])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "#         test = val # test on val\n",
    "\n",
    "        # Test\n",
    "        test_ids = [t['tweet_id'] for t in test] \n",
    "#         test_texts = [' '.join([str(i) for i in t['int_arr']]) for t in test] # treat as texts of numbers\n",
    "#         X_test = vectorizer.transform(test_texts)\n",
    "        y_test = [t['label'] for t in test]\n",
    "        tweet_embs, context_embs = [], []\n",
    "        for t_id in test_ids:\n",
    "                tweet_embs.append(contextifier.get_tweet_embedding(t_id))\n",
    "                context_embs.append(contextifier.get_context_embedding(t_id))\n",
    "                context_sizes[t_id] = len(contextifier.get_context_tweets(t_id)) # context size\n",
    "\n",
    "        X_test = hstack([csr_matrix(np.array(tweet_embs)), csr_matrix(np.array(context_embs))])\n",
    "#         X_test = hstack([csr_matrix(np.array(tweet_embs))])\n",
    "        y_predicted = clf.predict(X_test)\n",
    "\n",
    "        # Results\n",
    "        p, r, f, _ = skm.precision_recall_fscore_support(y_test, y_predicted, average='macro')\n",
    "        total_f += f\n",
    "\n",
    "    avg_f = total_f / 5\n",
    "    avg_context = sum(context_sizes.values())/len(context_sizes)\n",
    "\n",
    "    print('Avg F-score:', avg_f)\n",
    "    print('Avg number of context tweets in window:', avg_context)\n",
    "    print(params)\n",
    "    \n",
    "\n",
    "    if avg_f > best_f:\n",
    "        best_f = avg_f\n",
    "        best_params = params\n",
    "        best_context = avg_context\n",
    "\n",
    "\n",
    "\n",
    "print('BEST F:', best_f)\n",
    "print('BEST CONTEXT:', best_context)\n",
    "print('BEST PARAMS:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contextifier.get_context_embedding(832351449069846528))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
