{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.svm\n",
    "import sklearn.metrics as skm\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader ...\n",
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "from data_loader import Data_loader\n",
    "option = 'word'\n",
    "max_len = 53\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing TweetLevel...\n",
      "Number of word vectors in ../data/splex_minmax_svd_word_s300_seeds_hc.pkl: 20000\n",
      "Built tweet_dict. Sample tweet_dict item: (740043438788345856, [2, 254, 440, 192, 94, 57, 72, 77])\n",
      "Size of tweet_dict: 1033655\n",
      "Initializing TweetLevel...\n",
      "Number of word vectors in ../data/w2v_word_s300_w5_mc5_it20.bin: 23417\n",
      "Built tweet_dict. Sample tweet_dict item: (740043438788345856, [2, 254, 440, 192, 94, 57, 72, 77])\n",
      "Size of tweet_dict: 1033655\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from represent_context import Contextifier\n",
    "from represent_tweet_level import TweetLevel\n",
    "tl_splex = TweetLevel('../data/splex_minmax_svd_word_s300_seeds_hc.pkl')\n",
    "tl_word = TweetLevel('../data/w2v_word_s300_w5_mc5_it20.bin')\n",
    "post_types = [Contextifier.SELF, Contextifier.RETWEET, Contextifier.MENTION,\n",
    "                Contextifier.RETWEET_MENTION]\n",
    "context_size = 2\n",
    "context_hl_ratio = 0.5\n",
    "context_combine = 'avg'\n",
    "tl_combine = 'sum'\n",
    "\n",
    "# Create contextifier, feed it splex to get splex context\n",
    "contextifier = Contextifier(tl_splex, post_types, context_size,\n",
    "                            context_hl_ratio, context_combine, tl_combine)\n",
    "\n",
    "# Set context\n",
    "context = contextifier.assemble_context(dl.all_data())\n",
    "contextifier.set_context(*context)\n",
    "emb = contextifier.get_context_embedding(832351449069846528) # get the embedding for a tweet\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'vocab_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-62659b525361>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#         clf = sklearn.svm.LinearSVC() # no class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# with class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Training on both TR and VAL -- maybe a good idea?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vocab_size'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# param_grid = {'context_size': [0.1, 0.25, 0.5, 2, 7, 14],\n",
    "#               'use_rt_user': [True, False],\n",
    "#               'use_mentions': [True, False],\n",
    "#               'use_rt_mentions': [True, False],\n",
    "#               'context_hl_ratio': [0, 0.1, 0.25, 0.5], # relative to size\n",
    "#               'word_emb_file': ['../data/w2v_word_s300_w5_mc5_it20.bin'],\n",
    "#               'word_emb_type': ['w2v'],\n",
    "#               'word_emb_mode': ['avg'],\n",
    "#               'use_word_ct': [False],\n",
    "#               'splex_emb_file': ['../data/splex_minmax_svd_word_s300_seeds_hc.pkl'],\n",
    "#               'splex_emb_mode':['sum'],\n",
    "#               'use_splex_ct': [True],\n",
    "#               'keep_stats': [True]\n",
    "#              }\n",
    "\n",
    "param_grid = {'context_size': [2],\n",
    "              'post_types': [[Contextifier.SELF]],\n",
    "              'context_hl_ratio': [1],\n",
    "              'context_combine': ['sum'],\n",
    "              'tl_combine': ['sum']\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "best_f = 0\n",
    "best_params = None\n",
    "best_context = 0\n",
    "\n",
    "for params in grid:\n",
    "    contextifier.set_context_size(params['context_size'])\n",
    "    contextifier.set_post_types(params['post_types'])\n",
    "    contextifier.set_context_hl_ratio(params['context_hl_ratio'])\n",
    "    contextifier.set_context_combine(params['context_combine'])\n",
    "    contextifier.set_tl_combine(params['tl_combine'])\n",
    "    \n",
    "    total_f = 0\n",
    "    context_sizes = {}\n",
    "\n",
    "    class_weight = {\n",
    "        'Loss' : 0.35,\n",
    "        'Aggression': 0.5,\n",
    "        'Other': 0.15\n",
    "    }\n",
    "\n",
    "    for fold_idx in range(0, 5):\n",
    "    #     print('Fold:', fold_idx)\n",
    "        tr, val, test = dl.cv_data(fold_idx)\n",
    "\n",
    "        # Set up\n",
    "#         clf = sklearn.svm.LinearSVC() # no class weights\n",
    "        clf = sklearn.svm.LinearSVC(class_weight=class_weight) # with class weights\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 1), tokenizer=lambda s: s.split(' '))\n",
    "\n",
    "        # Training on both TR and VAL -- maybe a good idea?\n",
    "        all_train_tweets = [t for l in [tr, val] for t in l ]\n",
    "\n",
    "        # Train\n",
    "        train_ids = [t['tweet_id'] for t in all_train_tweets]\n",
    "#         train_texts = [' '.join([str(i) for i in t['int_arr']]) for t in all_train_tweets] # treat as texts of numbers\n",
    "#         X_train = vectorizer.fit_transform(train_texts)\n",
    "        y_train = [t['label'] for t in all_train_tweets]\n",
    "        word_embs, splex_embs, context_embs = [], [], []\n",
    "        for t_id in train_ids:\n",
    "                word_embs.append(tl_word.get_representation(t_id, 'avg'))\n",
    "                splex_embs.append(tl_splex.get_representation(t_id, 'sum'))\n",
    "                emb, ct_tweets = contextifier.get_context_embedding(t_id, keep_stats=True)\n",
    "                context_embs.append(emb)\n",
    "                context_sizes[t_id] = len(ct_tweets) #context size\n",
    "                \n",
    "#         X_train = hstack([csr_matrix(np.array(word_embs)), csr_matrix(np.array(splex_embs)),\n",
    "#                              csr_matrix(np.array(context_embs))])\n",
    "        X_train = hstack([csr_matrix(np.array(word_embs)), csr_matrix(np.array(splex_embs))])\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "#         test = val # test on val\n",
    "\n",
    "        # Test\n",
    "        test_ids = [t['tweet_id'] for t in test] \n",
    "#         test_texts = [' '.join([str(i) for i in t['int_arr']]) for t in test] # treat as texts of numbers\n",
    "#         X_test = vectorizer.transform(test_texts)\n",
    "        y_test = [t['label'] for t in test]\n",
    "        word_embs, splex_embs, context_embs = [], [], []\n",
    "        for t_id in test_ids:\n",
    "                word_embs.append(tl_word.get_representation(t_id, 'avg'))\n",
    "                splex_embs.append(tl_splex.get_representation(t_id, 'sum'))\n",
    "                emb, ct_tweets = contextifier.get_context_embedding(t_id, keep_stats=True)\n",
    "                context_embs.append(emb)\n",
    "                context_sizes[t_id] = len(ct_tweets) #context size\n",
    "\n",
    "#         X_test = hstack([csr_matrix(np.array(word_embs)), csr_matrix(np.array(splex_embs)),\n",
    "#                              csr_matrix(np.array(context_embs))])\n",
    "        X_test = hstack([csr_matrix(np.array(word_embs)), csr_matrix(np.array(splex_embs))])\n",
    "        y_predicted = clf.predict(X_test)\n",
    "\n",
    "        # Results\n",
    "        p, r, f, _ = skm.precision_recall_fscore_support(y_test, y_predicted, average='macro')\n",
    "        total_f += f\n",
    "\n",
    "    avg_f = total_f / 5\n",
    "    avg_context = sum(context_sizes.values())/len(context_sizes)\n",
    "\n",
    "    print('Avg F-score:', avg_f)\n",
    "    print('Avg number of context tweets in window:', avg_context)\n",
    "    print(params)\n",
    "    \n",
    "\n",
    "    if avg_f > best_f:\n",
    "        best_f = avg_f\n",
    "        best_params = params\n",
    "        best_context = avg_context\n",
    "\n",
    "\n",
    "\n",
    "print('BEST F:', best_f)\n",
    "print('BEST CONTEXT:', best_context)\n",
    "print('BEST PARAMS:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contextifier.get_context_embedding(832351449069846528))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
