{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_loader basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First you will need to download the \"data.pkl\" file from the google drive folder \"Datasets/pickle_04212018\" to the data directory. It contains all the data (labeled and unlabeled) in a transformed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data loader\n",
    "from data_loader import Data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# word level tokenization\n",
    "option = 'word'\n",
    "max_len = 20\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_idx = 0 # suppose that we want to load the cross validation data for fold 0\n",
    "tr, val, test = dl.cv_data(fold_idx) # get the cross validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each data point is now a dictionary\n",
      "------ a single data point ------\n",
      "tweet_id: 549982154667859968\n",
      "user_name: HeadHunchoMillz\n",
      "created_at: 2017-11-20 14:54:33.315932\n",
      "int_arr: [2, 16, 662, 709, 282, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label: Other\n"
     ]
    }
   ],
   "source": [
    "data_point0 = tr[0]\n",
    "print(\"Each data point is now a dictionary\")\n",
    "print(\"------ a single data point ------\")\n",
    "for key in data_point0:\n",
    "    print('%s: %s' % (str(key), str(data_point0[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@user u acting slow ðŸ˜“ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_\n"
     ]
    }
   ],
   "source": [
    "# I deliberatley deleted the unicode representation from the dictionaries\n",
    "# to avoid confusions\n",
    "# to access the unicode representation of a tweet\n",
    "# use the \"convert2unicode\" function\n",
    "s = dl.convert2unicode(data_point0['int_arr'])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1033655 data for unsupervised learning\n",
      "As before, each datapoint is a dictionary. However, it might have the \"label\" attribute.\n",
      "------ a single data point ------\n",
      "{'tweet_id': 905716028390481921, 'user_name': 'TGottiFrmLowe_', 'created_at': datetime.datetime(2017, 9, 7, 8, 55, 3), 'int_arr': [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "all_data = dl.all_data()\n",
    "print(\"There are %d data for unsupervised learning\" % len(all_data))\n",
    "print(\"As before, each datapoint is a dictionary. However, it might have the \\\"label\\\" attribute.\")\n",
    "print(\"------ a single data point ------\")\n",
    "print(all_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending vocab properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "# The current vocabulary lookup table is in model/word.pkl\n",
    "word2property = pkl.load(open('../model/word.pkl', 'rb'))\n",
    "print(word2property[b'wow'])\n",
    "print(word2property['ðŸ˜“'.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, word.pkl is a dictionary that maps from a **binary encoded** token to its property dictionary. Currently it contains information about: i) its id (index, in int_arr_rep) ii) occurence count in labeled/unlabled corpus iii) isemoji or not. \n",
    "\n",
    "In the future it might include word-embeddings/Splex scores, etc. To extend, simply load the dictionary, update, dump and commit. **Be sure not to change the other attributes, especially the lookup index; and update attribute for every single word.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending tweet properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pkl.load(open('../data/data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet_data = data['data']\n",
    "tweet = tweet_data[905716028390481921]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, data['data'] is a dictionary that maps a tweet_id to its correspdongin tweet attributes. To update, simply add an attribute to the tweet dictionary. **Be sure that all labeled tweets (tweets that have a \"label field\") have the same attributes.** (e.g. make sure that all a labeled tweets a have a context representation.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
