{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_loader basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First you will need to REDOWNLOAD the \"data.pkl\" file from the google drive folder \"Datasets/pickle_04212018\" to the data directory. It contains all the data (labeled and unlabeled) in a transformed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data loader\n",
    "from data_loader import Data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading user information finished\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# word level tokenization\n",
    "option = 'word'\n",
    "max_len = 20\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_idx = 0 # suppose that we want to load the cross validation data for fold 0\n",
    "tr, val, test = dl.cv_data(fold_idx) # get the cross validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each data point is now a dictionary\n",
      "------ a single data point ------\n",
      "{'tweet_id': 833942638038548480, 'user_mentions': [1527], 'int_arr': [2, 8, 1, 5408, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'label': 'Loss', 'created_at': datetime.datetime(2017, 11, 23, 12, 24, 29, 403482), 'user_post': 203}\n"
     ]
    }
   ],
   "source": [
    "data_point0 = tr[0]\n",
    "print(\"Each data point is now a dictionary\")\n",
    "print(\"------ a single data point ------\")\n",
    "print(data_point0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attributes: \n",
    "<br>*int_arr*: int representation of the tweet\n",
    "<br>*user_post*: the id of the user who posted the tweet\n",
    "<br>*label*: label of the tweet for classification\n",
    "<br>*user_mentions*: the set of user ids being mentioned\n",
    "<br>*tweet_id*: tweet_id of the tweet\n",
    "<br>*created_at*: datetime object when the tweet was posted\n",
    "<br>*retweet*: if this attribute exists, then the tweet was retweeted from the user id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info about a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_id: 833942638038548480\n",
      "user_mentions: [1527]\n",
      "int_arr: [2, 8, 1, 5408, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label: Loss\n",
      "created_at: 2017-11-23 12:24:29.403482\n",
      "user_post: 203\n",
      "User greedybandz posted the tweet.\n",
      "Users being mentioned: _shayauna\n",
      "original tweet content: @user my _UNKNOWN_ apologies _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_\n"
     ]
    }
   ],
   "source": [
    "# I deliberatley deleted the unicode representation from the dictionaries\n",
    "# to avoid confusions\n",
    "# to print the information about the tweet\n",
    "dl.print_recovered_tweet(data_point0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data indexed by user and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "tweet_id: 422958425316667392\n",
      "user_mentions: []\n",
      "int_arr: [92, 45, 72, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "label: Loss\n",
      "created_at: 2014-01-14 05:08:00\n",
      "user_post: 2\n",
      "User tyquanassassin posted the tweet.\n",
      "Users being mentioned: \n",
      "original tweet content: rip lil bro tyquan _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_\n",
      "-------------\n",
      "tweet_id: 422963500479045632\n",
      "user_mentions: []\n",
      "int_arr: [154, 459, 38, 188, 686, 62, 198, 794, 43, 46, 1074, 925, 5, 0, 0, 0, 0, 0, 0, 0]\n",
      "label: Other\n",
      "created_at: 2014-01-14 05:28:00\n",
      "user_post: 2\n",
      "User tyquanassassin posted the tweet.\n",
      "Users being mentioned: \n",
      "original tweet content: why dese niggas aint riding if dey part of da set ‚ùì üíØ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_\n"
     ]
    }
   ],
   "source": [
    "user_id = 2\n",
    "user_tweets = dl.tweets_by_user(user_id)\n",
    "for idx in range(2):\n",
    "    print('-------------')\n",
    "    dl.print_recovered_tweet(user_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1033655 data for unsupervised learning\n",
      "As before, each datapoint is a dictionary. However, it might have the \"label\" attribute.\n",
      "------ a single data point ------\n",
      "{'tweet_id': 905716028390481921, 'user_mentions': [], 'int_arr': [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'user_retweet': 1231, 'created_at': datetime.datetime(2017, 9, 7, 8, 55, 3), 'user_post': 44}\n",
      "tweet_id: 905716028390481921\n",
      "user_mentions: []\n",
      "int_arr: [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "user_retweet: 1231\n",
      "created_at: 2017-09-07 08:55:03\n",
      "user_post: 44\n",
      "User tgottifrmlowe_ posted the tweet.\n",
      "Users being mentioned: \n",
      "Retweet from shellywelly53.\n",
      "original tweet content: if u mine ... i go crazy behind you ü§ó _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_ _PAD_\n"
     ]
    }
   ],
   "source": [
    "all_data = dl.all_data()\n",
    "print(\"There are %d data for unsupervised learning\" % len(all_data))\n",
    "print(\"As before, each datapoint is a dictionary. However, it might have the \\\"label\\\" attribute.\")\n",
    "print(\"------ a single data point ------\")\n",
    "print(all_data[-1])\n",
    "dl.print_recovered_tweet(all_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending vocab properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isemoji': False, 'occurence_in_labeled': 4, 'id': 1875, 'occurence_in_unlabeled': 806}\n",
      "{'isemoji': True, 'occurence_in_labeled': 60, 'id': 282, 'occurence_in_unlabeled': 3608}\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "# The current vocabulary lookup table is in model/word.pkl\n",
    "word2property = pkl.load(open('../model/word.pkl', 'rb'))\n",
    "print(word2property[b'wow'])\n",
    "print(word2property['üòì'.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, word.pkl is a dictionary that maps from a **binary encoded** token to its property dictionary. Currently it contains information about: i) its id (index, in int_arr_rep) ii) occurence count in labeled/unlabled corpus iii) isemoji or not. \n",
    "\n",
    "In the future it might include word-embeddings/Splex scores, etc. To extend, simply load the dictionary, update, dump and commit. **Be sure not to change the other attributes, especially the lookup index; and update attribute for every single word.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending tweet properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pkl.load(open('../data/data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 905716028390481921, 'user_mentions': [], 'char_int_arr': [7, 21, 2, 11, 2, 15, 7, 9, 3, 2, 28, 28, 28, 2, 7, 2, 16, 5, 2, 19, 10, 6, 32, 14, 2, 18, 3, 13, 7, 9, 17, 2, 14, 5, 11, 2, 118], 'word_int_arr': [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319], 'user_retweet': 1231, 'created_at': datetime.datetime(2017, 9, 7, 8, 55, 3), 'user_post': 44}\n"
     ]
    }
   ],
   "source": [
    "tweet_data = data['data']\n",
    "tweet = tweet_data[905716028390481921]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, data['data'] is a dictionary that maps a tweet_id to its correspdongin tweet attributes. To update, simply add an attribute to the tweet dictionary. **Be sure that all labeled tweets (tweets that have a \"label field\") have the same attributes.** (e.g. make sure that all a labeled tweets a have a context representation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending user properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labeled_user_post': 426, 'unlabeled_user_post': 0, 'unlabeled_user_mentioned': 179, 'occurence_in_labeled': 787, 'labeled_user_mentioned': 361, 'unlabeled_user_rt': 224, 'id': 2, 'labeled_user_rt': 0, 'occurence_in_unlabeled': 403}\n"
     ]
    }
   ],
   "source": [
    "user2property = pkl.load(open('../model/user.pkl', 'rb'))\n",
    "user_info = user2property['tyquanassassin']\n",
    "print(user_info)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
