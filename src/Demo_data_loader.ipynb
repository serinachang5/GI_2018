{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_loader basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First you will need to REDOWNLOAD the \"data.pkl\" file from the google drive folder \"Datasets/pickle_04212018\" to the data directory. It contains all the data (labeled and unlabeled) in a transformed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loader ...\n",
      "Loading vocabulary ...\n",
      "1000 vocab is considered.\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "# import the data loader\n",
    "from data_loader import Data_loader\n",
    "# initialization\n",
    "# char level tokenization\n",
    "# YOU NEED TO CHANGE THIS ARGUMENT TO LOAD DIFFERENT TOKENIZATION LEVEL\n",
    "option = 'char'\n",
    "# you will have to change the vocab_size for 'char' option\n",
    "dl = Data_loader(option=option, vocab_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_idx = 0 # suppose that we want to load the cross validation data for fold 0\n",
    "# get the cross validation data\n",
    "#each of tr, val, test is a list\n",
    "tr, val, test = dl.cv_data(fold_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each data point is now a dictionary\n",
      "------ a single data point ------\n",
      "{'created_at': datetime.datetime(2017, 11, 20, 12, 18, 27, 645188), 'user_post': 215, 'user_mentions': [], 'padded_int_arr': [22, 19, 2, 18, 2, 28, 2, 46, 4, 5, 2, 45, 8, 9, 2, 33, 9, 9, 2, 29, 15, 4, 27, 3, 16, 2, 37, 6, 2, 45, 3, 2, 33, 8, 7, 32, 5, 2, 29, 15, 4, 27, 8, 7, 2, 35, 7, 2, 44, 4], 'int_arr': [22, 19, 2, 18, 2, 28, 2, 46, 4, 5, 2, 45, 8, 9, 2, 33, 9, 9, 2, 29, 15, 4, 27, 3, 16, 2, 37, 6, 2, 45, 3, 2, 33, 8, 7, 32, 5, 2, 29, 15, 4, 27, 8, 7, 2, 35, 7, 2, 44, 4, 11, 27, 9, 2, 85, 2, 115], 'tweet_id': 832508907998244865, 'label': 'Aggression', 'user_retweet': 23}\n"
     ]
    }
   ],
   "source": [
    "data_point0 = tr[0]\n",
    "print(\"Each data point is now a dictionary\")\n",
    "print(\"------ a single data point ------\")\n",
    "print(data_point0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attributes: \n",
    "<br>*int_arr*: int representation of the tweet\n",
    "<br>*user_post*: the id of the user who posted the tweet\n",
    "<br>*label*: label of the tweet for classification\n",
    "<br>*user_mentions*: the set of user ids being mentioned\n",
    "<br>*tweet_id*: tweet_id of the tweet\n",
    "<br>*created_at*: datetime object when the tweet was posted\n",
    "<br>*retweet*: if this attribute exists, then the tweet was retweeted from the user id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info about a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at: 2017-11-20 12:18:27.645188\n",
      "user_post: 215\n",
      "user_mentions: []\n",
      "padded_int_arr: [22, 19, 2, 18, 2, 28, 2, 46, 4, 5, 2, 45, 8, 9, 2, 33, 9, 9, 2, 29, 15, 4, 27, 3, 16, 2, 37, 6, 2, 45, 3, 2, 33, 8, 7, 32, 5, 2, 29, 15, 4, 27, 8, 7, 2, 35, 7, 2, 44, 4]\n",
      "int_arr: [22, 19, 2, 18, 2, 28, 2, 46, 4, 5, 2, 45, 8, 9, 2, 33, 9, 9, 2, 29, 15, 4, 27, 3, 16, 2, 37, 6, 2, 45, 3, 2, 33, 8, 7, 32, 5, 2, 29, 15, 4, 27, 8, 7, 2, 35, 7, 2, 44, 4, 11, 27, 9, 2, 85, 2, 115]\n",
      "tweet_id: 832508907998244865\n",
      "label: Aggression\n",
      "user_retweet: 23\n",
      "original tweet content: RT b'@user' : Got His Ass Smoked Na He Ain't Smokin On Folks b'\\xf0\\x9f\\xa4\\xa6' b'\\xf0\\x9f\\xa4\\xb7'\n"
     ]
    }
   ],
   "source": [
    "# I deliberatley deleted the unicode representation from the dictionaries\n",
    "# to avoid confusions\n",
    "# to print the information about the tweet\n",
    "dl.print_recovered_tweet(data_point0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data indexed by user and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "created_at: 2014-01-14 05:08:00\n",
      "user_post: 2\n",
      "user_mentions: []\n",
      "int_arr: [22, 8, 24, 2, 40, 8, 11, 2, 25, 12, 4, 2, 5, 13, 93, 14, 6, 7]\n",
      "tweet_id: 422958425316667392\n",
      "label: Loss\n",
      "padded_int_arr: [22, 8, 24, 2, 40, 8, 11, 2, 25, 12, 4, 2, 5, 13, 93, 14, 6, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "original tweet content: Rip Lil bro tyquan\n",
      "-------------\n",
      "created_at: 2014-01-14 05:28:00\n",
      "user_post: 2\n",
      "user_mentions: []\n",
      "int_arr: [42, 10, 13, 2, 39, 3, 9, 3, 2, 37, 8, 17, 17, 6, 9, 2, 33, 8, 7, 5, 2, 22, 8, 16, 8, 7, 17, 2, 21, 26, 2, 39, 3, 13, 2, 51, 6, 12, 5, 2, 35, 26, 2, 39, 6, 2, 29, 3, 5, 2, 220, 2, 43]\n",
      "tweet_id: 422963500479045632\n",
      "label: Other\n",
      "padded_int_arr: [42, 10, 13, 2, 39, 3, 9, 3, 2, 37, 8, 17, 17, 6, 9, 2, 33, 8, 7, 5, 2, 22, 8, 16, 8, 7, 17, 2, 21, 26, 2, 39, 3, 13, 2, 51, 6, 12, 5, 2, 35, 26, 2, 39, 6, 2, 29, 3, 5, 2]\n",
      "original tweet content: Why Dese Niggas Aint Riding If Dey Part Of Da Set b'\\xe2\\x9d\\x93' b'\\xf0\\x9f\\x92\\xaf'\n"
     ]
    }
   ],
   "source": [
    "user_id = 2\n",
    "user_tweets = dl.tweets_by_user(user_id)\n",
    "for idx in range(2):\n",
    "    print('-------------')\n",
    "    dl.print_recovered_tweet(user_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1033655 data for unsupervised learning\n",
      "As before, each datapoint is a dictionary. However, it might have the \"label\" attribute.\n",
      "------ a single data point ------\n",
      "{'created_at': datetime.datetime(2017, 9, 7, 8, 55, 3), 'user_mentions': [], 'user_post': 46, 'int_arr': [22, 19, 2, 18, 2, 28, 2, 21, 26, 2, 14, 2, 15, 8, 7, 3, 2, 38, 38, 38, 2, 8, 2, 17, 4, 2, 20, 12, 6, 54, 13, 2, 25, 3, 10, 8, 7, 16, 2, 13, 4, 14, 2, 157], 'tweet_id': 905716028390481921, 'padded_int_arr': [22, 19, 2, 18, 2, 28, 2, 21, 26, 2, 14, 2, 15, 8, 7, 3, 2, 38, 38, 38, 2, 8, 2, 17, 4, 2, 20, 12, 6, 54, 13, 2, 25, 3, 10, 8, 7, 16, 2, 13, 4, 14, 2, 157, 0, 0, 0, 0, 0, 0], 'user_retweet': 2704}\n",
      "created_at: 2017-09-07 08:55:03\n",
      "user_mentions: []\n",
      "user_post: 46\n",
      "int_arr: [22, 19, 2, 18, 2, 28, 2, 21, 26, 2, 14, 2, 15, 8, 7, 3, 2, 38, 38, 38, 2, 8, 2, 17, 4, 2, 20, 12, 6, 54, 13, 2, 25, 3, 10, 8, 7, 16, 2, 13, 4, 14, 2, 157]\n",
      "tweet_id: 905716028390481921\n",
      "padded_int_arr: [22, 19, 2, 18, 2, 28, 2, 21, 26, 2, 14, 2, 15, 8, 7, 3, 2, 38, 38, 38, 2, 8, 2, 17, 4, 2, 20, 12, 6, 54, 13, 2, 25, 3, 10, 8, 7, 16, 2, 13, 4, 14, 2, 157, 0, 0, 0, 0, 0, 0]\n",
      "user_retweet: 2704\n",
      "original tweet content: RT b'@user' : If u mine ... i go crazy behind you b'\\xf0\\x9f\\xa4\\x97'\n"
     ]
    }
   ],
   "source": [
    "all_data = dl.all_data()\n",
    "print(\"There are %d data for unsupervised learning\" % len(all_data))\n",
    "print(\"As before, each datapoint is a dictionary. However, it might have the \\\"label\\\" attribute.\")\n",
    "print(\"------ a single data point ------\")\n",
    "print(all_data[-1])\n",
    "dl.print_recovered_tweet(all_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New features demanded on 5/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of training set 818305: \n",
      "a tweet dictionary looks like\n",
      "{'created_at': datetime.datetime(2014, 10, 12, 17, 12, 52), 'user_mentions': [], 'user_post': 239, 'int_arr': [44, 8, 7, 7, 6, 2, 17, 12, 6, 25, 2, 9, 4, 15, 3, 2, 6, 20, 17, 9], 'tweet_id': 521347782691815424, 'padded_int_arr': [44, 8, 7, 7, 6, 2, 17, 12, 6, 25, 2, 9, 4, 15, 3, 2, 6, 20, 17, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "tr, val = dl.unlabeled_tr_val()\n",
    "# each of tr, val is a list of dictionaries\n",
    "print('size of training set %d: ' % len(tr))\n",
    "print('a tweet dictionary looks like')\n",
    "print(val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To get more \"formated\" info about a tweet dictionary, use the following function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created_at: 2014-10-12 17:12:52\n",
      "user_mentions: []\n",
      "user_post: 239\n",
      "int_arr: [44, 8, 7, 7, 6, 2, 17, 12, 6, 25, 2, 9, 4, 15, 3, 2, 6, 20, 17, 9]\n",
      "tweet_id: 521347782691815424\n",
      "padded_int_arr: [44, 8, 7, 7, 6, 2, 17, 12, 6, 25, 2, 9, 4, 15, 3, 2, 6, 20, 17, 9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "original tweet content: Finna grab some acgs\n"
     ]
    }
   ],
   "source": [
    "dl.print_recovered_tweet(val[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice that the option argument (cell 1) determines whether int_arr is in char level or word level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'occurence_in_labeled': 51, 'id': 135, 'isemoji': True, 'occurence_in_unlabeled': 3561}\n",
      "id = 135\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "# token2property is a map from token a property dictionary\n",
    "# notice that all words are in BINARY\n",
    "token2property = dl.token2property\n",
    "if option == 'word':\n",
    "    word = b'why'\n",
    "    print(token2property[word])\n",
    "    print('id = %d' % token2property[word]['id'])\n",
    "if option == 'char':\n",
    "    char = 'ðŸ˜“'.encode('utf-8')\n",
    "    print(token2property[char])\n",
    "    print('id = %d' % token2property[char]['id'])\n",
    "id2token = dl.id2token\n",
    "print(id2token[10])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
