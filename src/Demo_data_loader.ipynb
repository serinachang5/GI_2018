{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data_loader basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First you will need to REDOWNLOAD the \"data.pkl\" file from the google drive folder \"Datasets/pickle_04212018\" to the data directory. It contains all the data (labeled and unlabeled) in a transformed format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data loader\n",
    "from data_loader import Data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vocabulary ...\n",
      "30000 vocab is considered.\n",
      "Loading tweets ...\n",
      "Processing tweets ...\n",
      "Data loader initialization finishes\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "# word level tokenization\n",
    "option = 'word'\n",
    "max_len = 20\n",
    "vocab_size = 30000\n",
    "dl = Data_loader(vocab_size=vocab_size, max_len=max_len, option=option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fold_idx = 0 # suppose that we want to load the cross validation data for fold 0\n",
    "tr, val, test = dl.cv_data(fold_idx) # get the cross validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format of a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each data point is now a dictionary\n",
      "------ a single data point ------\n",
      "{'label': 'Other', 'int_arr': [2, 419], 'created_at': datetime.datetime(2017, 6, 14, 15, 58, 8, 290462), 'user_mentions': [1153], 'user_post': 4, 'tweet_id': 832351449069846528, 'padded_int_arr': [2, 419, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "data_point0 = tr[0]\n",
    "print(\"Each data point is now a dictionary\")\n",
    "print(\"------ a single data point ------\")\n",
    "print(data_point0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attributes: \n",
    "<br>*int_arr*: int representation of the tweet\n",
    "<br>*user_post*: the id of the user who posted the tweet\n",
    "<br>*label*: label of the tweet for classification\n",
    "<br>*user_mentions*: the set of user ids being mentioned\n",
    "<br>*tweet_id*: tweet_id of the tweet\n",
    "<br>*created_at*: datetime object when the tweet was posted\n",
    "<br>*retweet*: if this attribute exists, then the tweet was retweeted from the user id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get info about a data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: Other\n",
      "int_arr: [2, 419]\n",
      "created_at: 2017-06-14 15:58:08.290462\n",
      "user_mentions: [1153]\n",
      "user_post: 4\n",
      "tweet_id: 832351449069846528\n",
      "padded_int_arr: [2, 419, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "original tweet content: @user wassup\n"
     ]
    }
   ],
   "source": [
    "# I deliberatley deleted the unicode representation from the dictionaries\n",
    "# to avoid confusions\n",
    "# to print the information about the tweet\n",
    "dl.print_recovered_tweet(data_point0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access data indexed by user and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "label: Loss\n",
      "int_arr: [92, 45, 72, 549]\n",
      "created_at: 2014-01-14 05:08:00\n",
      "user_mentions: []\n",
      "user_post: 2\n",
      "tweet_id: 422958425316667392\n",
      "padded_int_arr: [92, 45, 72, 549, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "original tweet content: rip lil bro tyquan\n",
      "-------------\n",
      "label: Other\n",
      "int_arr: [154, 459, 38, 188, 686, 62, 198, 794, 43, 46, 1074, 925, 5]\n",
      "created_at: 2014-01-14 05:28:00\n",
      "user_mentions: []\n",
      "user_post: 2\n",
      "tweet_id: 422963500479045632\n",
      "padded_int_arr: [154, 459, 38, 188, 686, 62, 198, 794, 43, 46, 1074, 925, 5, 0, 0, 0, 0, 0, 0, 0]\n",
      "original tweet content: why dese niggas aint riding if dey part of da set ‚ùì üíØ\n"
     ]
    }
   ],
   "source": [
    "user_id = 2\n",
    "user_tweets = dl.tweets_by_user(user_id)\n",
    "for idx in range(2):\n",
    "    print('-------------')\n",
    "    dl.print_recovered_tweet(user_tweets[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1033655 data for unsupervised learning\n",
      "As before, each datapoint is a dictionary. However, it might have the \"label\" attribute.\n",
      "------ a single data point ------\n",
      "{'int_arr': [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319], 'tweet_id': 905716028390481921, 'user_mentions': [], 'user_post': 58, 'created_at': datetime.datetime(2017, 9, 7, 8, 55, 3), 'padded_int_arr': [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'user_retweet': 1296}\n",
      "int_arr: [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319]\n",
      "tweet_id: 905716028390481921\n",
      "user_mentions: []\n",
      "user_post: 58\n",
      "created_at: 2017-09-07 08:55:03\n",
      "padded_int_arr: [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "user_retweet: 1296\n",
      "original tweet content: if u mine ... i go crazy behind you ü§ó\n"
     ]
    }
   ],
   "source": [
    "all_data = dl.all_data()\n",
    "print(\"There are %d data for unsupervised learning\" % len(all_data))\n",
    "print(\"As before, each datapoint is a dictionary. However, it might have the \\\"label\\\" attribute.\")\n",
    "print(\"------ a single data point ------\")\n",
    "print(all_data[-1])\n",
    "dl.print_recovered_tweet(all_data[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending vocab properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'isemoji': False, 'occurence_in_unlabeled': 806, 'id': 1875, 'occurence_in_labeled': 4}\n",
      "{'isemoji': True, 'occurence_in_unlabeled': 3608, 'id': 282, 'occurence_in_labeled': 60}\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "# The current vocabulary lookup table is in model/word.pkl\n",
    "word2property = pkl.load(open('../model/word.pkl', 'rb'))\n",
    "print(word2property[b'wow'])\n",
    "print(word2property['üòì'.encode('utf-8')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, word.pkl is a dictionary that maps from a **binary encoded** token to its property dictionary. Currently it contains information about: i) its id (index, in int_arr_rep) ii) occurence count in labeled/unlabled corpus iii) isemoji or not. \n",
    "\n",
    "In the future it might include word-embeddings/Splex scores, etc. To extend, simply load the dictionary, update, dump and commit. **Be sure not to change the other attributes, especially the lookup index; and update attribute for every single word.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending tweet properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pkl.load(open('../data/data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tweet_id': 905716028390481921, 'user_mentions': [], 'user_post': 58, 'created_at': datetime.datetime(2017, 9, 7, 8, 55, 3), 'char_int_arr': [23, 19, 2, 18, 9, 5, 3, 8, 2, 28, 2, 21, 26, 2, 9, 2, 16, 11, 10, 3, 2, 37, 37, 37, 2, 11, 2, 15, 4, 2, 20, 8, 7, 54, 14, 2, 24, 3, 13, 11, 10, 17, 2, 14, 4, 9, 2, 142], 'word_int_arr': [62, 16, 911, 163, 4, 66, 151, 1474, 6, 319], 'user_retweet': 1296}\n"
     ]
    }
   ],
   "source": [
    "tweet_data = data['data']\n",
    "tweet = tweet_data[905716028390481921]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, data['data'] is a dictionary that maps a tweet_id to its correspdongin tweet attributes. To update, simply add an attribute to the tweet dictionary. **Be sure that all labeled tweets (tweets that have a \"label field\") have the same attributes.** (e.g. make sure that all a labeled tweets a have a context representation.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending user properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'tyquanassassin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-252d50418ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0muser2property\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../model/user.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muser_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser2property\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tyquanassassin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tyquanassassin'"
     ]
    }
   ],
   "source": [
    "user2property = pkl.load(open('../model/user.pkl', 'rb'))\n",
    "user_info = user2property['tyquanassassin']\n",
    "print(user_info)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
